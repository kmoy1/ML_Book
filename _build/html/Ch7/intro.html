
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Eigenvectors and Quadratic Forms &#8212; Machine Learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/quiz.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Anisotropic Gaussians" href="../Ch8/intro.html" />
    <link rel="prev" title="Gaussian Discriminant Analysis" href="../Ch6/intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ML_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/NNexample.html">
     Neural Network Backpropagation: An Example
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Neurobiology and Variations on Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/js-test.html">
   DEV ONLY: Test Interactive
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Ch7/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2FCh7/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quadratic-forms">
   Quadratic Forms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-a-quadratic">
   Building a Quadratic
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anisotropic-gaussian">
   Anisotropic Gaussian
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#covariance">
   Covariance
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="eigenvectors-and-quadratic-forms">
<h1>Eigenvectors and Quadratic Forms<a class="headerlink" href="#eigenvectors-and-quadratic-forms" title="Permalink to this headline">¶</a></h1>
<p>(Eigenvector review)</p>
<div class="section" id="quadratic-forms">
<h2>Quadratic Forms<a class="headerlink" href="#quadratic-forms" title="Permalink to this headline">¶</a></h2>
<p>Let’s talk about a way to <em>visualize</em> symmetric matrices. This way, we can gain intuition about what a matrix means, what linear transformations mean.</p>
<p>The <strong>quadratic form</strong> gives us hints about how applying the matrix affects the length of a vector.</p>
<p>Let’s say we have a vector <span class="math notranslate nohighlight">\(z\)</span>,. If we plot <span class="math notranslate nohighlight">\(||z||^2 = z^Tz\)</span> in space, we get a nice paraboloid.</p>
<p>What happens if we insert a (symmetric) matrix in the norm? Specifically, <span class="math notranslate nohighlight">\(||A^{-1}x||^2 = x^TA^{-2}x\)</span>? This is called the <strong>quadratic form</strong> of <span class="math notranslate nohighlight">\(A^{-2}\)</span>.  What does this look like?</p>
<p>Notably, we can use eigenvectors to transform <span class="math notranslate nohighlight">\(||z||^2\)</span> to <span class="math notranslate nohighlight">\(||A^{-1}x||^2\)</span>, and vice versa.</p>
<p>We know <span class="math notranslate nohighlight">\(z^Tz\)</span> is quadratic and isotropic: the curvature is the same in every direction, and all isosurfaces are spheres.</p>
<p>On the other hand, <span class="math notranslate nohighlight">\(x^TA^{-2}x\)</span> is  NOT isotropic in general, unless <span class="math notranslate nohighlight">\(A\)</span> is some multiple of the identity matrix. So its isosurfaces will be ellipsoids.</p>
<p>So fundamentally we have two different spaces: <span class="math notranslate nohighlight">\(z\)</span>-space and <span class="math notranslate nohighlight">\(x\)</span>-space. We <strong>transform</strong> from one space to another by transformation matrix <span class="math notranslate nohighlight">\(A\)</span>, specifically, through the equation <span class="math notranslate nohighlight">\(x = Az\)</span>.</p>
<p>Let’s say <span class="math notranslate nohighlight">\(A = \begin{bmatrix}
3/4 &amp; 5/4\\
5/4 &amp; 3/4
\end{bmatrix}\)</span>. Now here’s the really interesting part: <strong>the eigenvectors match the axes of the ellipse in its contour plot.</strong></p>
<img alt="../_images/contour1.png" src="../_images/contour1.png" />
<p>So given we start off with unit vectors pointing off in the same direction as eigenvectors <span class="math notranslate nohighlight">\(v_1, v_2\)</span> in <span class="math notranslate nohighlight">\(z\)</span>-space:</p>
<img alt="../_images/contour2.png" src="../_images/contour2.png" />
<p>After applying <span class="math notranslate nohighlight">\(A\)</span>, one vector gets mapped to a vector twice as long (corresponding to eigenvector 2), and another is halved  and negated (eigenvector -1/2).</p>
<p>The way A maps a vector applies to <em>any</em> point on the isocontours. In other words, a point in <span class="math notranslate nohighlight">\(z\)</span>-space gets mapped to a new point in <span class="math notranslate nohighlight">\(x\)</span>-space using the eigenvalue as a multiplier. However, the eigenvalues themselves stay the same!</p>
<p>In general, matrix <span class="math notranslate nohighlight">\(A\)</span> maps the (unit) circle in <span class="math notranslate nohighlight">\(z\)</span>-space to the corresponding ellipse in <span class="math notranslate nohighlight">\(x\)</span>-space. Let’s try to verify this formally.</p>
<p>If we draw the contour plot for <span class="math notranslate nohighlight">\(||A^{-1}x||^2 = 1\)</span>, and look at the contour line with isovalue 1, that is the equation of an ellipsoid with axes  as eigenvectors <span class="math notranslate nohighlight">\(v_1,...v_n\)</span> and radii <span class="math notranslate nohighlight">\(\lambda_1,...,\lambda_n\)</span>. This is true because if <span class="math notranslate nohighlight">\(A^{-1}x = v_i\)</span> where <span class="math notranslate nohighlight">\(v_i\)</span> is length 1, that means that <span class="math notranslate nohighlight">\(v_i\)</span> lies on the unit circle, and thus means that <span class="math notranslate nohighlight">\(x = Av_i\)</span> will have length <span class="math notranslate nohighlight">\(\lambda_i\)</span>. This ultimately means that <span class="math notranslate nohighlight">\(Av_i\)</span> lies on the ellipsoid.</p>
<p>A special case arises when <span class="math notranslate nohighlight">\(A\)</span> is diagonal. This means the eigenvectors will be the coordinate axes- the ellipsoid is <strong>axis-aligned.</strong></p>
<p>Consider a symmetric matrix <span class="math notranslate nohighlight">\(M\)</span>. It is <strong>positive definite</strong> if <span class="math notranslate nohighlight">\(w^TMw &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(w \neq 0\)</span>. Equivalently, a symmetric matrix is PD if <strong>all its eigenvalues are positive</strong>. A <strong>positive semidefinite</strong> matrix is when <span class="math notranslate nohighlight">\(w^TMw \ge 0\)</span>, or equivalently, all eigenvalues of <span class="math notranslate nohighlight">\(M\)</span> are nonnegative. Finally, a symmetric matrix is <strong>indefinite</strong> if it has both one positive eigenvalue AND one negative eigenvalue.</p>
<p>So now we can represent <span class="math notranslate nohighlight">\(||A^{-1}x||^2 = x^TA^{-2}x\)</span> as <span class="math notranslate nohighlight">\(x^TMx\)</span>, where <span class="math notranslate nohighlight">\(M = A^{-2}\)</span>. <span class="math notranslate nohighlight">\(M=A^{-2}\)</span> must be positive definite: it is squared and zero eigenvalues cannot exist.</p>
<p>What about the <strong>isosurfaces</strong> of <span class="math notranslate nohighlight">\(x^TMx\)</span> for a positive definite matrix <span class="math notranslate nohighlight">\(M\)</span>? The contour plot of an <span class="math notranslate nohighlight">\(n\)</span>-th quadratic form will have ellipsoidal isosurfaces, with radii determined by eigenvalues determined by <span class="math notranslate nohighlight">\(A = M^{-1/2}\)</span>. The eigenvalues of <span class="math notranslate nohighlight">\(M^{-1/2}\)</span> are just the eigenvalues raised to the negative 1/2 themselves.</p>
<p>There is another case when <span class="math notranslate nohighlight">\(M\)</span> is only positive semidefinite. The isosurfaces generally aren’t ellipsoids any more: they are <strong>cylinders</strong>.</p>
<p>So the general rule is this: you’ll be given <span class="math notranslate nohighlight">\(x^TA^{-2}x\)</span>, and want some geometric intuition. Getting the eigenvalues of <span class="math notranslate nohighlight">\(A = M^{-1/2}\)</span> will give us the geometric eigenvectors + radii we want. In the case of the <strong>anisotropic Gaussian</strong>, an important term is <span class="math notranslate nohighlight">\(x^T\Sigma^{-1}x\)</span>: so we get <span class="math notranslate nohighlight">\(A = \Sigma^{1/2}\)</span>.</p>
</div>
<div class="section" id="building-a-quadratic">
<h2>Building a Quadratic<a class="headerlink" href="#building-a-quadratic" title="Permalink to this headline">¶</a></h2>
<p>Given eigenvectors and eigenvalues, we can construct a matrix with those eigenvectors/values using <strong>singular value decomposition</strong> (SVD). It has something to do with <span class="math notranslate nohighlight">\(A = V \Lambda V^T = \sum_{i=1}^{n}\lambda_iv_iv_i^T\)</span>. This is called the <strong>outer product matrix</strong>. Each summation has rank 1, and each term contributes one eigenvector to the sum. So the end sum has all the eigenvectors and values we’ve asked for!</p>
<p><span class="math notranslate nohighlight">\(A = V \Lambda V^T\)</span> is a matrix factorization called <strong>eigendecomposition</strong>. <strong>Every real symmetric matrix has an eigendecomposition</strong>.</p>
<p>We can think of <span class="math notranslate nohighlight">\(\Lambda\)</span> as a <strong>diagonalized version of <span class="math notranslate nohighlight">\(A\)</span></strong>: it has stretching terms via eigenvalues, but in the coordinate system of the eigenvectors this time.</p>
<p>This means that <span class="math notranslate nohighlight">\(V^T\)</span> is a matrix that <strong>rotates</strong> the ellipsoid to be axis-aligned.</p>
<p>So <span class="math notranslate nohighlight">\(Az\)</span> can be broken down into 3 steps: <span class="math notranslate nohighlight">\(V\Lambda V^Tz\)</span>. The first transformation <span class="math notranslate nohighlight">\(V^Tz\)</span> <em>rotates</em> the coordinate system so the eigenvectors are now aligned with the coordinate system. The applying <span class="math notranslate nohighlight">\(\Lambda\)</span> stretches/shrinks along those new axes. Finally,  <span class="math notranslate nohighlight">\(V\)</span> reverses the rotation back to the original coordinate system.</p>
<p>So now that we’ve chosen our ellipsoid axes/radii as eigenvectors/values, then getting those axes/radii from a matrix is just a matter of taking the quadratic form of <span class="math notranslate nohighlight">\(A^{-2}\)</span>. So we just take <span class="math notranslate nohighlight">\(||A^{-1}x||^2 = 1\)</span> or <span class="math notranslate nohighlight">\(x^TA^{-2}x = 1\)</span> (these are isocontours).</p>
<p>So we’ve specified a paraboloid whose isosurfaces are ellipsoids with specified axes and radii.</p>
<p>Note that <span class="math notranslate nohighlight">\(A^2 = V\Lambda^2V^T\)</span>. So when we square a matrix, we square the eigenvalues while <em>retaining the same exact eigenvectors.</em></p>
<p>So given a symmetric PSD matrix, like the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> of a multivariate Gaussian distribution, we can find the <strong>symmetric square root by taking the eigendecomposition:</strong></p>
<ol class="simple">
<li><p>Compute eigenvectors/vals of <span class="math notranslate nohighlight">\(\Sigma\)</span></p></li>
<li><p>Take square roots of <span class="math notranslate nohighlight">\(\Sigma\)</span>’s eigenvalues.</p></li>
<li><p>Reassemble matrix <span class="math notranslate nohighlight">\(A\)</span> with the same eigenvectors but square-rooted eigenvalues.</p></li>
</ol>
</div>
<div class="section" id="anisotropic-gaussian">
<h2>Anisotropic Gaussian<a class="headerlink" href="#anisotropic-gaussian" title="Permalink to this headline">¶</a></h2>
<p>Now, <span class="math notranslate nohighlight">\(X \sim N(\mu, \Sigma)\)</span> allows different variances along different directions. Remember <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span> are <span class="math notranslate nohighlight">\(d\)</span>-dimensional vectors, while <span class="math notranslate nohighlight">\(\Sigma\)</span> is a <span class="math notranslate nohighlight">\(d \times d\)</span> matrix. The PDF of this distribution:</p>
<!-- TODO: Write out PDF of an anisotropic Gaussian -->
<p><span class="math notranslate nohighlight">\(\Sigma\)</span> is a PSD covariance matrix, and <span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span> is a PSD <strong>precision matrix</strong>.</p>
<p>Now let’s write our density as a composition of two functions; <span class="math notranslate nohighlight">\(f(x) = n(q(x))\)</span>. <span class="math notranslate nohighlight">\(q(x) = (x-\mu)^T\Sigma^{-1}(x-\mu)\)</span>. <span class="math notranslate nohighlight">\(n\)</span> is a scalar exponential mapping, while <span class="math notranslate nohighlight">\(q\)</span> is a vector-to-scalar quadratic mapping.</p>
<p>The important part: <span class="math notranslate nohighlight">\(n\)</span> DOES NOT CHANGE THE ISOSURFACES OF <span class="math notranslate nohighlight">\(\Sigma\)</span>. Generally, <strong>given a monotonic <span class="math notranslate nohighlight">\(n\)</span> that maps <span class="math notranslate nohighlight">\(R \to R\)</span>, isosurfaces of <span class="math notranslate nohighlight">\(n(q(x))\)</span> are the same as <span class="math notranslate nohighlight">\(q(x)\)</span>.</strong> (Isovalues will be different, however).</p>
<p>Look at <span class="math notranslate nohighlight">\(n\)</span>:</p>
<img alt="../_images/nplot.png" src="../_images/nplot.png" />
</div>
<div class="section" id="covariance">
<h2>Covariance<a class="headerlink" href="#covariance" title="Permalink to this headline">¶</a></h2>
<p>Consider <span class="math notranslate nohighlight">\(R,S\)</span> as two random variables- either vectors OR scalars. The covariance of <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(S\)</span> is defined as</p>
<p>$<span class="math notranslate nohighlight">\(\text{Cov}(R,S) = E[(R-E[R])(S-E[S])^T] = E[RS^T] - \mu_R\mu_S^T\)</span>$.</p>
<p>If <span class="math notranslate nohighlight">\(R_i, R_j\)</span> are independent, then <span class="math notranslate nohighlight">\(\text{Cov}(R_i, R_j) = 0\)</span>. The converse is NOT generally true.</p>
<p>If all features of <span class="math notranslate nohighlight">\(R\)</span> are pairwise independent, then <span class="math notranslate nohighlight">\(\text{Var}(R)\)</span> is diagonal.</p>
<p>Var(R) is diagonal AND joint normal iff the Gaussian itself is axis-aligned.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Ch7"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../Ch6/intro.html" title="previous page">Gaussian Discriminant Analysis</a>
    <a class='right-next' id="next-link" href="../Ch8/intro.html" title="next page">Anisotropic Gaussians</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kevin Moy<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>