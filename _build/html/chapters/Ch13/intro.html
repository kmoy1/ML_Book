
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Kernels &#8212; Machine Learning</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mathjax.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Neural Networks" href="../Ch14/intro.html" />
    <link rel="prev" title="Lecture 15: Decision Trees Cont." href="../Ch12/intro2.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/ML_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machapters/Chines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/intro2.html">
     Neural Network Variations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch19/intro.html">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch20/intro.html">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch22/intro.html">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch13/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch13/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-magic-kernelization">
   The Magic: Kernelization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-perceptrons">
   Kernel Perceptrons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernelized-logistic-regression">
   Kernelized Logistic Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-kernel">
   Gaussian Kernel
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="kernels">
<h1>Kernels<a class="headerlink" href="#kernels" title="Permalink to this headline">¶</a></h1>
<p>One of the tasks we’re now familiar with is feature engineering: we transform raw data into feature vectors through a feature map. This process can take a long ass time, especially if we decide to have a ton of features. The kernel trick, or kernelization, can make things so we don’t need our feature map, but just a <em>similarity</em> map between datapoints.</p>
<p>Say we have a dataset with <span class="math notranslate nohighlight">\(d\)</span> features and we want to fit a degree-<span class="math notranslate nohighlight">\(p\)</span> polynomial decision boundary to them. Of course, overfitting is always an issue here. But we will also be using way more features- specifically, <span class="math notranslate nohighlight">\(O(d^p)\)</span> features for a degree-<span class="math notranslate nohighlight">\(p\)</span> polynomial with <span class="math notranslate nohighlight">\(d\)</span> features in our dataset! For example, if we have <span class="math notranslate nohighlight">\(d = 100\)</span> features and want to fit a degree <span class="math notranslate nohighlight">\(p = 4\)</span> decision function- each lifted feature vector needs to account for <em>all combinations</em> of features, and thus will have about <em>four million</em> features. Not good.</p>
<p>With kernelization, we’ll find we can still compute this feature vector incredibly fast; we don’t need to compute all features explicitly to use them.</p>
<p>Kernelization is based on two observations about many learning algorithms.</p>
<ol class="simple">
<li><p>The optimal solution for an optomization problem can often be written as a linear combination of the sample points. We see this in a lot of algorithms: SVMs, ridge regression, perceptrons, logistic regression,  etc.</p></li>
<li><p>We can denote optimal solutions as inner products of <strong>lifted feature vectors</strong>. For a sample point <span class="math notranslate nohighlight">\(x\)</span>,  <em>lifted feature vector</em> is denoted as <span class="math notranslate nohighlight">\(\Phi(x)\)</span>. Many optomization algorithms only require computation of the inner product of few <span class="math notranslate nohighlight">\(\Phi(x)\)</span> vectors as their central computation. Often <strong>we don’t need to compute <span class="math notranslate nohighlight">\(\Phi(x)\)</span> to find their inner product</strong>!</p></li>
</ol>
<p>Let’s take a mathematical look into observation 1. Let’s suppose optimal weight vector <span class="math notranslate nohighlight">\(w\)</span> (could be many) takes the form:</p>
<p><span class="math notranslate nohighlight">\(w = X^Ta = \sum_{i=1}^{n}a_iX_i\)</span></p>
<p>where <span class="math notranslate nohighlight">\(X\)</span> is our design matrix, and <span class="math notranslate nohighlight">\(a\)</span> is our coefficient vector that indicates the optimal linear combination of sample points. If we assume this, we can instead the optimal <span class="math notranslate nohighlight">\(a\)</span> instead of <span class="math notranslate nohighlight">\(w\)</span>. In practice, we simply substitute <span class="math notranslate nohighlight">\(w = X^Ta\)</span> into our algorithm and solve for <span class="math notranslate nohighlight">\(a\)</span>. <span class="math notranslate nohighlight">\(a\)</span>’s weights are called <strong>dual weights</strong>. So now, our goal is to find <span class="math notranslate nohighlight">\(n\)</span> optimal dual weights for <span class="math notranslate nohighlight">\(a\)</span> instead of <span class="math notranslate nohighlight">\(d+1\)</span> (or <span class="math notranslate nohighlight">\(d^p\)</span>) optimal primal weights for <span class="math notranslate nohighlight">\(w\)</span>.</p>
<p>Let’s do this for ridge regression as an example. Before we dualize the ridge regression equation, though, there’s one small detail we need to take care of first. In this case, we do want to penalize the bias term because we’re finding a linear combination of the sample points- this is only true IF we penalize the bias term. To minimize damage from this, we first center <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span> so their means are zero. Specifically, for each sample point (row) <span class="math notranslate nohighlight">\(X_i\)</span>, subtract the mean <span class="math notranslate nohighlight">\(\mu_X\)</span>. For each <span class="math notranslate nohighlight">\(y_i\)</span>, subtract by <span class="math notranslate nohighlight">\(\mu_y\)</span>. <em>Don’t center the bias column, though: a 0s column will not be useful.</em></p>
<p>With centered <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, it’s less harmful to penalize our bias term. So in the ridge regression equation, instead of <span class="math notranslate nohighlight">\(I'\)</span> where the last diagonal element of <span class="math notranslate nohighlight">\(I'\)</span> was 0, we can just use the identity matrix <span class="math notranslate nohighlight">\(I\)</span> in our normal equations. Recall our normal equations for the standard primal form of ridge regression:</p>
<p><span class="math notranslate nohighlight">\((X^TX + \lambda I)w = X^Ty\)</span></p>
<p>Why does centering help? If we have data from a random distribution, then the expected linear regression will pass through the origin. So centering <span class="math notranslate nohighlight">\(X,y\)</span> will have the decision boundary likely to pass through the origin or close to it.</p>
<p>Let’s now look at the normal equations for the dual form of ridge regression, which we can obtain from above via substitution. Suppose we have a vector <span class="math notranslate nohighlight">\(a\)</span> which is a solution to:</p>
<p><span class="math notranslate nohighlight">\((XX^T+\lambda I)a = y\)</span></p>
<p>Then, <span class="math notranslate nohighlight">\(X^Ty = X^TXX^Ta + \lambda X^Ta = (X^TX+\lambda I)X^T a\)</span>. Therefore, <span class="math notranslate nohighlight">\(w = X^Ta\)</span> is a solution to our normal equations, and <span class="math notranslate nohighlight">\(w\)</span> is a linear combination of points in <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>So we call <span class="math notranslate nohighlight">\(a\)</span> the <strong>dual solution</strong>. It solves the dual form of ridge regression and we want to find <span class="math notranslate nohighlight">\(a\)</span> that minimizes</p>
<p><span class="math notranslate nohighlight">\(||XX^Ta-y||^2+\lambda||X^Ta||^2\)</span></p>
<p>We got this objective function by simply plugging in <span class="math notranslate nohighlight">\(w = X^Ta\)</span> into the original cost function for ridge regression. We can easily verify <span class="math notranslate nohighlight">\(a\)</span> by taking the gradient of this function and solving for <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p>For the training part of dual ridge regression, we first solve the dual normal equations for <span class="math notranslate nohighlight">\(a\)</span>. Since we know <span class="math notranslate nohighlight">\((XX^T+\lambda I)\)</span> is symmetric and positive definite, it has a unique solution. For testing, our regression function on test point <span class="math notranslate nohighlight">\(z\)</span> is <span class="math notranslate nohighlight">\(h(z) = w^Tz = a^TXz\)</span>. Note that <span class="math notranslate nohighlight">\(a^TXz\)</span> is also a linear combination, or weighted sum, of inner products:</p>
<p><span class="math notranslate nohighlight">\(a^TXz = \sum_{i=1}^{n}a_iX_i^Tz\)</span></p>
<p>So our hypothesis is a weighted sum of inner products. The inner product <span class="math notranslate nohighlight">\(X_i^Tz\)</span> is an inner product here that will be <strong>very</strong> important for what we want to do later.</p>
<p>Now that we have a good understanding of dual ridge regression, let’s define a little terminology.</p>
<p>First, let us define the <strong>kernel function</strong> <span class="math notranslate nohighlight">\(k(x,z) = x^Tz\)</span>: simply the dot product of its input vectors. Later, we’ll replace <span class="math notranslate nohighlight">\(x, z\)</span> with <span class="math notranslate nohighlight">\(\Phi(x), \Phi(z)\)</span>, but we’re not there yet.</p>
<p>Now, define <strong>kernel matrix</strong> <span class="math notranslate nohighlight">\(K = XX^T\)</span>, where <span class="math notranslate nohighlight">\(K\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix. Note that <span class="math notranslate nohighlight">\(X\)</span> <em>does</em> have the bias dimension. Here, we can see that <span class="math notranslate nohighlight">\(K_{ij} = k(X_i, X_j)\)</span>. <span class="math notranslate nohighlight">\(K\)</span> is always positive semidefinite, but not necessarily positive definite. It is quite common for <span class="math notranslate nohighlight">\(K\)</span> to be singular, and this is quite common if <span class="math notranslate nohighlight">\(n &gt; d+1\)</span>- and singularity can even happen even when this is not the case. If <span class="math notranslate nohighlight">\(K\)</span> is singular, don’t expect a solution to dual ridge regression if <span class="math notranslate nohighlight">\(\lambda = 0\)</span> (no regularization). This means we probably want <em>some</em> regularization- and this is good anyway to reduce overfitting.</p>
<p>Kernelization and kernel algorithms is most interesting when <span class="math notranslate nohighlight">\(d\)</span> is very large, since our lifting map adds a lot of new features anyway. So the  <span class="math notranslate nohighlight">\(n &gt; d+1\)</span> isn’t something <em>too</em> worrisome.</p>
<p>Now let’s write out the dual ridge regression algorithm in a manner that uses the kernel matrix <span class="math notranslate nohighlight">\(K\)</span> and kernel function <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="proof algorithm admonition" id="algorithm-0">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span> (Dual Ridge Regression, Kernelized)</p>
<div class="algorithm-content section" id="proof-content">
<ol class="simple">
<li><p>Calculate kernel matrix <span class="math notranslate nohighlight">\(K\)</span> where <span class="math notranslate nohighlight">\(K_{ij} = k(X_i, X_j)\)</span>.</p></li>
<li><p>Solve dual ridge regression equation <span class="math notranslate nohighlight">\((K + \lambda I)a = y\)</span> for dual weights vector <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>For each test point <span class="math notranslate nohighlight">\(z\)</span></p>
<ol class="simple">
<li><p>Calculate prediction <span class="math notranslate nohighlight">\(h(z) = \sum_{i=1}^{n}a_ik(X_i, z)\)</span></p></li>
</ol>
</li>
</ol>
</div>
</div><p>Let’s calculate the runtime of our dual algorithm. First, in calculating <span class="math notranslate nohighlight">\(K\)</span>, we calculate <span class="math notranslate nohighlight">\(n^2\)</span> entries as <span class="math notranslate nohighlight">\(k(X_i, X_j)\)</span>. Each calculation of <span class="math notranslate nohighlight">\(k(X_i, X_j)\)</span> is the dot product which has <span class="math notranslate nohighlight">\(O(d)\)</span> runtime, so computing <span class="math notranslate nohighlight">\(K\)</span> is <span class="math notranslate nohighlight">\(O(n^2d)\)</span>. Then, solving our <span class="math notranslate nohighlight">\(n \times n\)</span> linear system of equations generally takes <span class="math notranslate nohighlight">\(O(n^3)\)</span> time. Finally, for each test point, we compute <span class="math notranslate nohighlight">\(h(z)\)</span>, which takes <span class="math notranslate nohighlight">\(O(nd)\)</span> time. So overall, since we’re considering <span class="math notranslate nohighlight">\(d &gt;&gt; n\)</span>, __the dual algorithm takes <span class="math notranslate nohighlight">\(O(n^2d + n^3)\)</span> time. __</p>
<p>Now, we’re going to work some magic to make our kernel function <span class="math notranslate nohighlight">\(k\)</span> <em>supremely fast</em>.</p>
<p>Note our dual algorithm <em>does not use sample points <span class="math notranslate nohighlight">\(X_i\)</span> directly</em>. It is only used as an input to our kernel function <span class="math notranslate nohighlight">\(k\)</span>. So this means if we can configure our kernel function <span class="math notranslate nohighlight">\(k\)</span> to avoid using <span class="math notranslate nohighlight">\(X_i\)</span>’s value directly, this will be great for speed.</p>
<p>Now let’s compare the dual algorithm with the primal for ridge regression. In the dual, we solve an <span class="math notranslate nohighlight">\(n \times n\)</span> linear system, and in the primal, we solve a <span class="math notranslate nohighlight">\(d \times d\)</span> system. We don’t transpose in the primal, so <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(d\)</span> simply swap places- meaning the <strong>primal algorithm has a <span class="math notranslate nohighlight">\(O(d^3 + d^2n)\)</span> runtime.</strong></p>
<p>This means that the choice between dual or primal depends on comparing <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(n\)</span>. For raw runtime, we prefer dual when <span class="math notranslate nohighlight">\(d &gt; n\)</span>, and primal when <span class="math notranslate nohighlight">\(d \leq n\)</span>. Practically, though, we’ll usually have way more sample points than features: <span class="math notranslate nohighlight">\(n &gt;&gt; d\)</span>. However, with polynomial features and parabolic lifting, we’re actually gonna have way more features than we think. <strong>More importantly, adding polynomial terms as new features will blow up <span class="math notranslate nohighlight">\(d\)</span> in the primal algorithm runtime, but does not affect <span class="math notranslate nohighlight">\(d\)</span> in the dual runtime.</strong> So dual ridge regression is looking better and better here.</p>
<p>Finally, remember that <strong>dual and primal produce the same exact predictions</strong>. They are just different ways of doing the same computation.</p>
<div class="section" id="the-magic-kernelization">
<h2>The Magic: Kernelization<a class="headerlink" href="#the-magic-kernelization" title="Permalink to this headline">¶</a></h2>
<p>Now finally, the magic part. We can compute a polynomial kernel with many monomial terms <em>without actually computing the individual terms itself</em>.</p>
<p>The polynomial kernel of degree <span class="math notranslate nohighlight">\(p\)</span> is given as <span class="math notranslate nohighlight">\(k(x,z) = (x^Tz + 1)^p\)</span>. Note that <span class="math notranslate nohighlight">\(x^Tz + 1\)</span> is a <em>scalar</em>, so taking it to a power is <span class="math notranslate nohighlight">\(O(1)\)</span> time.</p>
<p>A theorem: <span class="math notranslate nohighlight">\((x^Tz + 1)^p = \Phi(x)^T\Phi(z)\)</span> where <span class="math notranslate nohighlight">\(\Phi(x)\)</span> contains every monomial in <span class="math notranslate nohighlight">\(x\)</span> of degree <span class="math notranslate nohighlight">\(p\)</span> (degree 0 to p). For example, let’s say we have <span class="math notranslate nohighlight">\(d=2\)</span> features, and want a degree <span class="math notranslate nohighlight">\(p=2\)</span> polynomial. Let’s assume <span class="math notranslate nohighlight">\(x, z \in R^2\)</span>. Then,</p>
<p><span class="math notranslate nohighlight">\(k(x,z) = (x^Tz + 1)^2 = x_1^2z_1^2 + x_2^2z_2^2 + 2x_1z_1x_2z_2 + 2x_1z_1 + 2x_2z_2 + 1\)</span>.</p>
<p>We can factor this into an inner product of two vectors, one with <span class="math notranslate nohighlight">\(x\)</span> terms and one with <span class="math notranslate nohighlight">\(z\)</span> terms:</p>
<p><span class="math notranslate nohighlight">\(= \begin{bmatrix}x_1^2 &amp; x_2^2 &amp; \sqrt{2}x_1x_2 &amp; \sqrt{2}x_1 &amp; \sqrt{2}x_2 1\end{bmatrix} * \begin{bmatrix}z_1^2 &amp; z_2^2 &amp; \sqrt{2}z_1z_2 &amp; \sqrt{2}z_1 &amp; \sqrt{2}z_2 1\end{bmatrix}^T\)</span></p>
<p>Now define <span class="math notranslate nohighlight">\(\Phi(x)\)</span> and <span class="math notranslate nohighlight">\(\Phi(z)\)</span> as these two respective vectors. Which means we can <em>finally</em> compute <span class="math notranslate nohighlight">\(k(x,z) = \Phi(x)^T\Phi(z)\)</span> like we dreamed of, and calculate it as a single expression <span class="math notranslate nohighlight">\((x^Tz + 1)^p\)</span> instead of actually computing <span class="math notranslate nohighlight">\(\Phi(x)\)</span> or <span class="math notranslate nohighlight">\(\Phi(z)\)</span> themselves! It’ll take <span class="math notranslate nohighlight">\(O(d)\)</span> runtime instead of <span class="math notranslate nohighlight">\(O(d^p)\)</span> time (by calculating <span class="math notranslate nohighlight">\(d^p\)</span> terms in <span class="math notranslate nohighlight">\(\Phi(x)\)</span>).</p>
<p>Now, applying kernelization to ridge regression, the important thing to understand: we take our dual and replace <span class="math notranslate nohighlight">\(X_i\)</span> with <span class="math notranslate nohighlight">\(\Phi(X_i)\)</span>. Now, our kernel function is <span class="math notranslate nohighlight">\(k(x,z) = \Phi(x)^T\Phi(z)\)</span>. But we aren’t computing  <span class="math notranslate nohighlight">\(\Phi(x)\)</span> or <span class="math notranslate nohighlight">\(\Phi(z)\)</span> and taking their inner product. We are computing <span class="math notranslate nohighlight">\((x^Tz + 1)^p\)</span>, which is equivalent.</p>
</div>
<div class="section" id="kernel-perceptrons">
<h2>Kernel Perceptrons<a class="headerlink" href="#kernel-perceptrons" title="Permalink to this headline">¶</a></h2>
<p>So how can we apply kernelization to ML algorithms? Let’s look at an example with perceptrons. The perceptron algorithm will stay the same as before, except we replace <span class="math notranslate nohighlight">\(X_i\)</span> with <span class="math notranslate nohighlight">\(\Phi(X_i)\)</span>:</p>
<div class="proof algorithm admonition" id="algorithm-1">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Kernelized Perceptron, Primal)</p>
<div class="algorithm-content section" id="proof-content">
<ol class="simple">
<li><p>Pick some starting point, e.g. <span class="math notranslate nohighlight">\(w = y_1\Phi(X_1)\)</span></p></li>
<li><p>Training. While some <span class="math notranslate nohighlight">\(y_i \Phi(X_i) \cdot w &lt; 0\)</span> (there exists a misclassified point):</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(w \leftarrow w + \epsilon y_i \Phi(X_i)\)</span></p></li>
</ol>
</li>
<li><p>Testing. For each test point <span class="math notranslate nohighlight">\(z\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(h(z) \leftarrow w \cdot \Phi(z)\)</span></p></li>
</ol>
</li>
</ol>
</div>
</div><p>How do we convert this algorithm to its dual form?</p>
<p>First, we want a <em>lifted</em> design matrix <span class="math notranslate nohighlight">\(\Phi(X)\)</span>, which is an <span class="math notranslate nohighlight">\(n \times D\)</span> matrix where <span class="math notranslate nohighlight">\(D\)</span> is the number of features after lifting. Each row <span class="math notranslate nohighlight">\(\Phi(X_i)^T\)</span> is a lifted sample point.</p>
<p>Next, we define a kernel matrix <span class="math notranslate nohighlight">\(K = \Phi(X) \Phi(X)^T\)</span>. Note it is applied on lifted features, not raw features.</p>
<p>Next, we substitute <span class="math notranslate nohighlight">\(w = \Phi(X)^Ta\)</span>, noting that our optimal solution is simply a linear combination of lifted sample points. We solve for <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p>So in the primal algorithm’s training phase, in step 2.1, we use stochastic gradient descent to find an optimal <span class="math notranslate nohighlight">\(w\)</span>. In the dual algorithm, we update each weight in <span class="math notranslate nohighlight">\(a\)</span> individually as follows:</p>
<div class="math notranslate nohighlight">
\[
a_i \leftarrow a_i + \epsilon y_i
\]</div>
<p>This has the same effect as <span class="math notranslate nohighlight">\(w \leftarrow w + \epsilon y_i \Phi(X_i)\)</span> in the primal. We can verify this by simply substituting <span class="math notranslate nohighlight">\(w = \Phi(X)^Ta\)</span> into the primal update.</p>
<p>Also note that <span class="math notranslate nohighlight">\(\Phi(X_i) \cdot w = (\Phi(X) \cdot w)_i = (\Phi(X) \Phi(X)^T a)_i = (Ka)_i\)</span>.</p>
<p>Finally, we can set up our dual algorithm:</p>
<div class="proof algorithm admonition" id="algorithm-2">
<p class="admonition-title"><span class="caption-number">Algorithm 9 </span> (Kernelized Perceptron, Dual)</p>
<div class="algorithm-content section" id="proof-content">
<ol class="simple">
<li><p>Get a starting point <span class="math notranslate nohighlight">\(a\)</span>, e.g. <span class="math notranslate nohighlight">\(a = \begin{bmatrix}y_1 &amp; 0 &amp; ... &amp; 0 \end{bmatrix}^T\)</span></p></li>
<li><p>Compute kernel matrix <span class="math notranslate nohighlight">\(K\)</span></p></li>
<li><p>Training: While some <span class="math notranslate nohighlight">\(y_i(Ka)_i &lt; 0\)</span>:</p>
<ol class="simple">
<li><p>$a_i \leftarrow a_i + \epsilon y_i</p></li>
</ol>
</li>
<li><p>Testing: For each test point <span class="math notranslate nohighlight">\(z\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(h(z) \leftarrow \sum_{i=1}^{n}a_ik(X_i, z)\)</span></p></li>
</ol>
</li>
</ol>
</div>
</div><p>Let’s go over runtime again. Like always, computing the kernel matrix <span class="math notranslate nohighlight">\(K\)</span> takes <span class="math notranslate nohighlight">\(O(n^2d)\)</span> time. Note <span class="math notranslate nohighlight">\(d\)</span> is the number of <em>pre-lifted</em> (raw) features. Updating <span class="math notranslate nohighlight">\(a_i\)</span> takes <span class="math notranslate nohighlight">\(O(1)\)</span> time, but updating <span class="math notranslate nohighlight">\((Ka)_i\)</span> at every iteration takes <span class="math notranslate nohighlight">\(O(n)\)</span> time. Finally, computing test point predictions for <span class="math notranslate nohighlight">\(n\)</span> points takes <span class="math notranslate nohighlight">\(O(nd)\)</span> time.</p>
</div>
<div class="section" id="kernelized-logistic-regression">
<h2>Kernelized Logistic Regression<a class="headerlink" href="#kernelized-logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>Kernelization is <em>very</em> common in logistic regression, and we’ll find that we’ve already done most of the work in previous sections! The key differences just deal with the characteristics of logistic regression itself:</p>
<ol class="simple">
<li><p>Instead of searching for a misclassified sample point each iteration, we apply GD to all points at once, or random sample points.</p></li>
<li><p>The stochastic gradient step changes to account for logistic regression: it is now <span class="math notranslate nohighlight">\(a_i \leftarrow a_i + \epsilon(y_i - s((Ka)_i))\)</span>, where <span class="math notranslate nohighlight">\(s\)</span> is the logistic function.</p></li>
<li><p>Our starting weights vector is <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
</ol>
<p>Just like with perceptrons, every training iteration, we update a single dual weight <span class="math notranslate nohighlight">\(a_i\)</span>, and then update <span class="math notranslate nohighlight">\(Ka\)</span> in <span class="math notranslate nohighlight">\(O(n)\)</span> time. This way, we don’t have to compute <span class="math notranslate nohighlight">\(Ka\)</span> from scratch on the next iteration.</p>
<p>What if we want batch gradient descent instead? All we have to change here is now apply <span class="math notranslate nohighlight">\(s\)</span> element-wise to a vector, specifically vector <span class="math notranslate nohighlight">\(Ka\)</span>. So our update rule is <span class="math notranslate nohighlight">\(a \leftarrow a + \epsilon(y-s(Ka))\)</span>. And for test predictions, we simply apply the sigmoid function on our weighted sample point sum: <span class="math notranslate nohighlight">\(h(z) = s(\sum_{i=1}^{n}a_ik(X_i, z))\)</span>. Note this is purely for knowledge of posterior probabilities; if we just want the classifier prediction, we don’t really need the sigmoid function in any case, and can just take the weighted sum as usual.</p>
</div>
<div class="section" id="gaussian-kernel">
<h2>Gaussian Kernel<a class="headerlink" href="#gaussian-kernel" title="Permalink to this headline">¶</a></h2>
<p>So we’ve learned how to do fast computation in extremely high dimensions. So one day some crackhead decided to expand on this and ask: if we can work with high-dimensional feature vectors, what about feature vectors in <em>infinite</em>-dimensional space?</p>
<p>The <strong>Gaussian kernel</strong> is denoted by</p>
<div class="math notranslate nohighlight">
\[
k(x, z) = \exp(-\frac{||x-z||^2}{2\sigma^2})
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(-\frac{||x-z||^2}{2\sigma^2}\)</span> is the exponential part of the Gaussian PDF. Computing this kernel term takes <span class="math notranslate nohighlight">\(O(d)\)</span> time, as usual. Again, this kernel is the inner product of lifted feature vectors <span class="math notranslate nohighlight">\(\Phi(x) \cdot \Phi(z)\)</span>, where <span class="math notranslate nohighlight">\(\Phi\)</span> now has <em>infinite</em> dimensions.</p>
<p>Let’s consider the case when <span class="math notranslate nohighlight">\(d=1\)</span>. In this case, our lifted feature vector is</p>
<div class="math notranslate nohighlight">
\[
\Phi(X) = \exp(-\frac{x^2}{2\sigma^2})\begin{bmatrix} 1 &amp; \frac{x}{\sigma \sqrt{1}} &amp; \frac{x^2}{\sigma^2 \sqrt{2!}} &amp; \frac{x^3}{\sigma^3 \sqrt{3!}} &amp; ... \end{bmatrix}^T
\]</div>
<p>Again, we <em>don’t actually compute this</em>: instead, we are computing the kernel function <span class="math notranslate nohighlight">\(k(.,.)\)</span>.</p>
<p>Now the hypothesis function for infinite-dimension lifts would be <span class="math notranslate nohighlight">\(h(z) = \sum_{i=1}^{n}a_ik(X_i, z)\)</span>, which is a <em>linear combination of Gaussians</em>. Each Gaussian is centered at a sample point <span class="math notranslate nohighlight">\(X_i\)</span>. Let’s visualize this as a 3D plot of <span class="math notranslate nohighlight">\(h(z)\)</span>:</p>
<a class="reference internal image-reference" href="../../_images/Gaussians.png"><img alt="../../_images/Gaussians.png" class="align-center" src="../../_images/Gaussians.png" style="width: 400px;" /></a>
<p>Here we see a linear combination of four Gaussians. The 2 humps are Gaussians with <em>positive</em> coefficients <span class="math notranslate nohighlight">\(a_i\)</span>, while the 2 troughs are Gaussians with negative coefficients. Gaussian-kernel ridge regression will output this kind of regression function.</p>
<p>The Gaussian kernel is very popular in practice. Why? There are many reasons:</p>
<ul class="simple">
<li><p>Smooth regression function <span class="math notranslate nohighlight">\(h\)</span>: in fact, <span class="math notranslate nohighlight">\(h\)</span> is infinitely differentiable.</p></li>
<li><p>Behaves similar to k-nearest-neighbors. Sample points <span class="math notranslate nohighlight">\(X_i\)</span> closer to a test point <span class="math notranslate nohighlight">\(z\)</span> have a much greater influence than sample points further away. This is, of course, indicated by the kernel function. Fortunately, unlike k-NN, the Gaussian kernel tends to give much smoother decision boundaries.</p></li>
<li><p>Decision boundaries oscillate much less than high-degree polynomials, depending on Gaussian width hyperparameter <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(k(x,z)\)</span> is interpreted as a similarity function. Thus, <span class="math notranslate nohighlight">\(k\)</span> is maximized when <span class="math notranslate nohighlight">\(x = z\)</span>, but fades to 0 as the distance between <span class="math notranslate nohighlight">\(x, z\)</span> increases.</p></li>
</ul>
<p>The hyperparameter <span class="math notranslate nohighlight">\(\sigma\)</span> represents Gaussian width. Like all hyperparameters, we choose <span class="math notranslate nohighlight">\(\sigma\)</span> by validation. With larger <span class="math notranslate nohighlight">\(\sigma\)</span>, we have wider Gaussians, which means <span class="math notranslate nohighlight">\(h\)</span> is smoother, which increases bias but decreases variance.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch13"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../Ch12/intro2.html" title="previous page">Lecture 15: Decision Trees Cont.</a>
    <a class='right-next' id="next-link" href="../Ch14/intro.html" title="next page">Neural Networks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kevin Moy<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>