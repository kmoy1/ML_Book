
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Multiple Eigenvectors and Random Projection &#8212; Machine Learning</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mathjax.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Boosting" href="../Ch20/intro.html" />
    <link rel="prev" title="Spectral Graph Clustering" href="../Ch18/intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/ML_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machapters/Chines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/intro2.html">
     Neural Network Variations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch20/intro.html">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch22/intro.html">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch19/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch19/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering-with-multiple-eigenvectors">
   Clustering with Multiple Eigenvectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#geometry-of-high-dimensional-spaces">
   Geometry of High-Dimensional Spaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#angles-between-random-vectors">
   Angles Between Random Vectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-projection">
   Random Projection
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="multiple-eigenvectors-and-random-projection">
<h1>Multiple Eigenvectors and Random Projection<a class="headerlink" href="#multiple-eigenvectors-and-random-projection" title="Permalink to this headline">¶</a></h1>
<div class="section" id="clustering-with-multiple-eigenvectors">
<h2>Clustering with Multiple Eigenvectors<a class="headerlink" href="#clustering-with-multiple-eigenvectors" title="Permalink to this headline">¶</a></h2>
<p>So in the last chapter, we learned how to divide a graph <span class="math notranslate nohighlight">\(G\)</span> into two subgraphs/clusters, and divide those subgraphs recursively if we want more clusters. But there’s a better way. The <em>Fiedler vector</em> <span class="math notranslate nohighlight">\(v_2\)</span> tells us how to divide such graphs. However, there are several other methods to subdivide a graph into <span class="math notranslate nohighlight">\(k\)</span> clusters in one shot that use <em>multiple</em> eigenvectors. These methods use <span class="math notranslate nohighlight">\(k\)</span> eigenvectors in a natural way to cluster a graph into <span class="math notranslate nohighlight">\(k\)</span> subgraphs.</p>
<p>The first thing we do is set up our generalized eigensystem <span class="math notranslate nohighlight">\(Lv = \lambda Mv\)</span>. From this, we calculate the first <span class="math notranslate nohighlight">\(k\)</span> eigenvector solutions <span class="math notranslate nohighlight">\(v_1 = 1, v_2,...,v_k\)</span>. Then, we scale them such that <span class="math notranslate nohighlight">\(v_iMv_i = 1\)</span>. Now, <span class="math notranslate nohighlight">\(V^TMV = I\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> has columns <span class="math notranslate nohighlight">\(v_i\)</span> as the eigenvectors with the <span class="math notranslate nohighlight">\(k\)</span> <em>smallest</em> eigenvalues. Such normalized eigenvectors are called <strong>M-orthogonal</strong>.</p>
<img alt="../../_images/matrixV.png" src="../../_images/matrixV.png" />
<!-- TODO: Finish this section. -->
</div>
<div class="section" id="geometry-of-high-dimensional-spaces">
<h2>Geometry of High-Dimensional Spaces<a class="headerlink" href="#geometry-of-high-dimensional-spaces" title="Permalink to this headline">¶</a></h2>
<p>While many concepts from the familiar 2D and 3D geometry carry over to higher dimensions, high-dimensional geometry often has weird properties as well.</p>
<p>For example, consider a random point <span class="math notranslate nohighlight">\(p \sim N(0, I) \in \mathbb{R}^d\)</span>. What is the distribution of its <em>length</em>?</p>
<p>If <span class="math notranslate nohighlight">\(d=1\)</span>, we have the one-dimensional standard normal distribution we are so familiar with. In this case, the distribution of the length of <span class="math notranslate nohighlight">\(p\)</span> would just be <span class="math notranslate nohighlight">\(p\)</span>’s value itself: most commonly close to 0, and less and less common the further out from 0.</p>
<img alt="../../_images/normal.png" src="../../_images/normal.png" />
<p>Unfortunately, this concept <em>absolutely does not</em> carry over to higher dimensions. In high dimensions, <strong>most vectors (points) are at approximately the same distance from the mean</strong>. So most points <span class="math notranslate nohighlight">\(p\)</span> will actually lie on a <em>thin shell</em>.</p>
<p>To prove this, let’s look at the square of the distance. By Pythagoras’ Theorem, the squared distance from p to the mean is <span class="math notranslate nohighlight">\(||p||^2 = p_1^2 + p_2^2 + ... + p_d^2\)</span>, where <em>each</em> <span class="math notranslate nohighlight">\(p_i\)</span> is sampled at random from a (univariate) normal distribution. The square of this (<span class="math notranslate nohighlight">\(p_i^2\)</span>) is now a <em>chi-squared</em> distribution.</p>
<p>When you add <span class="math notranslate nohighlight">\(d\)</span> I.I.D random variables, you scale their mean and variance by <span class="math notranslate nohighlight">\(d\)</span>. So this means:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
E[||p||^2] = dE[p_1^2] = d \\
\text{Var}(||p||^2) = d\text{Var}(p_1^2) = 2d \\
\text{SD}(||p||^2) = \sqrt{2d}
\end{split}\]</div>
<p>For large <span class="math notranslate nohighlight">\(d\)</span>, <span class="math notranslate nohighlight">\(||p||\)</span> (length of random vectors) is concentrated in a thin shell around radius <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> with thickness proportional to <span class="math notranslate nohighlight">\((2d)^{\frac{1}{4}}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The mean value of <span class="math notranslate nohighlight">\(||p||\)</span> isn’t exactly <span class="math notranslate nohighlight">\(\sqrt{d}\)</span>, but it is close, because the mean of <span class="math notranslate nohighlight">\(||p||^2\)</span> is <span class="math notranslate nohighlight">\(d\)</span> and the standard
deviation is much, much smaller. Likewise, the standard deviation of <span class="math notranslate nohighlight">\(||p||\)</span> isn’t exactly  <span class="math notranslate nohighlight">\((2d)^{\frac{1}{4}}\)</span>, but it’s close.</p>
</div>
<p>Let’s say <span class="math notranslate nohighlight">\(d = 10^6\)</span>. This makes a million-dimensional egg with radius 1000, and the thickness of the eggshell is about 67, which is about 10 times the standard deviation. The vast majority of random points are in the thickness of this eggshell- NOT inside the egg!</p>
<p>There is a statistical principle hiding here. Suppose you want to estimate the mean of a distribution (chi-squared distribution in this case). Normally, you’d take a lot of samples and calculate the mean. The more numbers you sample, the smaller your standard deviation, and the more accurate your mean (probably) is. Guess what- calculating the length of a random vector from a million-dimensional normal distribution is exactly that!</p>
<p>We looked at a normal distribution. What about uniform? Consider concentric spheres of radii <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(r-\epsilon\)</span>:</p>
<img alt="../../_images/concentric.png" src="../../_images/concentric.png" />
<p>In high dimensions, almost every point chosen uniformly at random in the outer ball lies outside the inner (blue) ball.</p>
<p>The volume of the outer ball is proportional to <span class="math notranslate nohighlight">\(r^d\)</span>, while the inner ball has volume proportional to <span class="math notranslate nohighlight">\((r-\epsilon)^d\)</span>. So the proportion of total volume (<span class="math notranslate nohighlight">\(r^d\)</span>) taken up by the inner ball is given by <span class="math notranslate nohighlight">\(\frac{(r-\epsilon)^d}{r^d} = (1 - \frac{\epsilon}{r})^d \approx \exp(-\frac{\epsilon d}{r})\)</span>. This proportion goes to 0 as <span class="math notranslate nohighlight">\(d\)</span> gets very large. For example, if <span class="math notranslate nohighlight">\(\frac{\epsilon}{r} = 0.1\)</span> and <span class="math notranslate nohighlight">\(d = 100\)</span>, the inner ball has <span class="math notranslate nohighlight">\(0.9^{100} = 0.0027%\)</span> of volume.</p>
<p>So this means that for high dimensions, random points are nearly <em>all</em> in the outer shell. Note this is similar to normal distributions in high dimensions, where they all simply lie in a thin shell. What does this mean in general? In high dimensions, sometimes the nearest neighbor and 1,000th-nearest neighbor don’t differ much. Which means <span class="math notranslate nohighlight">\(k\)</span>-means and nearest-neighbors are less and less effective for large <span class="math notranslate nohighlight">\(d\)</span>.</p>
</div>
<div class="section" id="angles-between-random-vectors">
<h2>Angles Between Random Vectors<a class="headerlink" href="#angles-between-random-vectors" title="Permalink to this headline">¶</a></h2>
<p>What is the angle between two random vectors <span class="math notranslate nohighlight">\(p, q \in \mathbb{R}^d\)</span>?</p>
<p>WLOG set <span class="math notranslate nohighlight">\(q = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; ... &amp; 0 \end{bmatrix} ^T\)</span>. The actual value of <span class="math notranslate nohighlight">\(q\)</span> doesn’t matter because <span class="math notranslate nohighlight">\(p\)</span> is uniformly distributed over all possible directions anyway. We know the angle <span class="math notranslate nohighlight">\(\theta\)</span> between two vectors is <span class="math notranslate nohighlight">\(\cos \theta = \frac{p \cdot q}{||p|| ||q||}\)</span>. In this case, with our <span class="math notranslate nohighlight">\(q\)</span>, we have <span class="math notranslate nohighlight">\(\cos \theta = \frac{p_1}{||p||}\)</span>.</p>
<p>So <span class="math notranslate nohighlight">\(E[\cos \theta] \approx \frac{1}{\sqrt{d}}\)</span>. If <span class="math notranslate nohighlight">\(d\)</span> is large, then <span class="math notranslate nohighlight">\(\cos \theta \approx 0\)</span>, which means <span class="math notranslate nohighlight">\(\theta \approx 90^{\circ}\)</span>. This means in high-dimensional spaces, two random vectors are almost always close to orthogonal- a high-dimensional vector is orthogonal to the vast majority of all other high-dimensional vectors.</p>
</div>
<div class="section" id="random-projection">
<h2>Random Projection<a class="headerlink" href="#random-projection" title="Permalink to this headline">¶</a></h2>
<p><strong>Random projection</strong> is an alternative to PCA as a preprocessing tool. The main advantage it has is that it preserves distance between points.</p>
<p>In PCA, we project our original datapoints onto the <em>PCA subspace</em>- in random projection, we project, well, onto a <em>random subspace</em>. Random projection works best when projecting a very high-dimensional space to a medium-dimensional space. Because distance is (roughly) preserved, algorithms like k-means clustering and nearest neighbor classifiers will behave similarly but of course run much faster (since it’s lower dimensions).</p>
<p>How does it work? Well, the algorithm is complicated, but I’ll try to briefly explain it here.</p>
<!-- TODO: Explain random projection. -->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch19"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../Ch18/intro.html" title="previous page">Spectral Graph Clustering</a>
    <a class='right-next' id="next-link" href="../Ch20/intro.html" title="next page">Boosting</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kevin Moy<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>