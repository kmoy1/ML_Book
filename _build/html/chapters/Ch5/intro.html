
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Decision Theory &#8212; Machine Learning</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mathjax.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Gaussian Discriminant Analysis" href="../Ch6/intro.html" />
    <link rel="prev" title="Optomization" href="../Ch4/intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/ML_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machapters/Chines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/intro2.html">
     Neural Network Variations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch19/intro.html">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch20/intro.html">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch22/intro.html">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch5/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch5/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intro">
   Intro
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-cancer-diagnosis">
   Example: Cancer Diagnosis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-theorem">
   Bayes’ Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#risk-is-expected-loss">
   Risk is Expected Loss
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-classifier">
   Bayes Classifier
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#risk-in-continuous-distributions">
   Risk in Continuous Distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-classifier-in-higher-dimensions">
   Bayesian Classifier in Higher Dimensions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ways-to-build-classifiers">
   3 Ways to Build Classifiers
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="decision-theory">
<h1>Decision Theory<a class="headerlink" href="#decision-theory" title="Permalink to this headline">¶</a></h1>
<div class="section" id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Permalink to this headline">¶</a></h2>
<p>So far, we just covered classifiers that made a hard binary prediction: either a data point belonged in a class or it didn’t. But there can be real-world noise and distortions in data that can seriously mess this up. For example, we can have duplicate points in different classes: imagine two people, both making 50K with 10K credit balance, but one defaults and the other doesn’t. So we obviously won’t be able to have a linear classifier that classifies with 100% accuracy.</p>
<p>But what if we could make a classifier that told us a little more; instead of just the prediction itself, what if we could give the <em>probability</em> that a point belongs to a class? In other words, we would know the probability of a correct prediction for each data point! Such <strong>probabilistic classifiers</strong> are deeply rooted in <em>decision theory</em>, and see many uses in real life, from spam detection to medical diagnosis.</p>
</div>
<div class="section" id="example-cancer-diagnosis">
<h2>Example: Cancer Diagnosis<a class="headerlink" href="#example-cancer-diagnosis" title="Permalink to this headline">¶</a></h2>
<p>A very common example to start with: say we want to build a probabilistic classifer that predicts whether someone has AIDS or not. Let’s say 10% of the population has AIDS (and 90% doesn’t). This is known as the <strong>prior probability</strong>: the probability of diagnosis <em>before</em> any extra information is known. We’ll denote the prior of having cancer as <span class="math notranslate nohighlight">\(P(Y=1)\)</span>. From this, the prior of having no cancer is <span class="math notranslate nohighlight">\(P(Y=0) = 1 - P(Y=1)\)</span>.</p>
<p>Now, let’s say we want to predicting a person’s probability of having AIDS <em>after</em> receiving information. For simplicity, we’ll keep it at one feature: calorie intake. Specifically, we want to calculate <span class="math notranslate nohighlight">\(P(Y=1|X)\)</span>: the probability a person has cancer <em>given</em> some information <span class="math notranslate nohighlight">\(X\)</span>. This term <span class="math notranslate nohighlight">\(P(Y|X)\)</span> often called the <strong>likelihood</strong> of <span class="math notranslate nohighlight">\(Y\)</span> (cancer)- this is technically incorrect</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Obviously in real life a diagnosis would take into account many features, but the concept will extend from one to multiple features very easily anyway.</p>
</div>
<p>Now say we grab a bunch of people with various daily calorie intakes, which we divide into three ranges:</p>
<ul class="simple">
<li><p>Less than 1200 calories</p></li>
<li><p>1200-1600 calories</p></li>
<li><p>Greater than 1600 calories</p></li>
</ul>
<p>From each of these three groups, we measure what proportion of them actually have cancer, giving us the below table:</p>
<a class="reference internal image-reference" href="../../_images/calorieintake.png"><img alt="Calorie Intake Table" class="align-center" src="../../_images/calorieintake.png" style="width: 400px;" /></a>
<p>Remember that the probability of evidence <span class="math notranslate nohighlight">\(P(X)\)</span> can be expanded to</p>
<div class="math notranslate nohighlight">
\[
P(X) = P(X|Y=1)P(Y=1) + P(X|Y=0)P(Y=0)
\]</div>
<p>by law of total probability. For example, given the above chart, the probability that a given person from our sample eats less than 1200 calories can be calculated as <span class="math notranslate nohighlight">\(P(X) = P(X|Y=1)P(Y=1) + P(X|Y=0)P(Y=0) = 0.2 \times 0.1 + 0.1 \times 0.9 = 0.11\)</span>.</p>
<p>So back to the original point: we want to predict the <strong>posterior probability</strong> of a person getting cancer given their calorie intake: <span class="math notranslate nohighlight">\(P(Y=1|X)\)</span>. Let’s say we have a guy with a daily calorie intake of 1400 calories. We see that 50 percent of people with cancer have calorie intakes between 1200 and 1600. Do we conclude this patient probably has cancer? No. This reasoning fails to take prior probabilities into account- the fact that cancer itself is rare at 10 percent.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Posterior probabilities are generally interchanged with the term <strong>likelihood</strong>, although this is technically incorrect: likelihood refers to the probability of a certain model given observed data, where in this case we are predicting the probability of a <em>class prediction</em> given observed data.</p>
</div>
<p>So how can we accurately calculate <span class="math notranslate nohighlight">\(P(Y=1|X=1400)\)</span>?</p>
</div>
<div class="section" id="bayes-theorem">
<h2>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this headline">¶</a></h2>
<p>The correct way to calculate this is to use Bayes’ theorem, which <em>does</em> take prior probabilities into account. Remember that Bayes’ Theorem states that</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\]</div>
<p>which means we can formulate ourposterior probability, as</p>
<div class="math notranslate nohighlight">
\[
P(Y=1|X=1400) = \frac{P(X=1400|Y=1)P(Y=1)}{P(X=1400)} 
\]</div>
<p>which we can just solve using our table as <span class="math notranslate nohighlight">\(\frac{0.05}{0.14} = 0.357\)</span>, or a 35.7% chance the patient has cancer. So contrary to what we said before, our probability of cancer for the 1400-calorie patient is a good amount less than 50%.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The terms <span class="math notranslate nohighlight">\(P(X|Y=1)\)</span> and <span class="math notranslate nohighlight">\(P(X|Y=0)\)</span> are called the <strong>class-conditional distributions</strong> of <span class="math notranslate nohighlight">\(X\)</span>: they represent the probability distribution of <span class="math notranslate nohighlight">\(X\)</span> conditioned on class <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</div>
<p>One thing to always consider, though, is the importance of false negatives vs. false positives. Because having cancer is such a serious issue, a wrong false negative prediction (incorrectly predicting a patient does NOT have cancer) is much worse than a false positive prediction (incorrectly predicting that a patient DOES have cancer). Thus, we probably want fewer false negatives than false positives. So we might lower our threshold to predict positive, to, say, above 30% posterior probability.</p>
<p>We can represent such a tradeoff between false negatives and positives with our loss function <span class="math notranslate nohighlight">\(L(z,y)\)</span>. Before, our loss function simply output 1 for incorrect predictions and 0 for correct- this is called a <strong>0-1 loss function</strong>. Here, false negatives and false positives are weighted the same. However, we can certainly adjust this loss function to penalize false negative predictions more! For example, we can set <span class="math notranslate nohighlight">\(L\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L(z, y) = \begin{cases} 
            1 &amp; z=1, y=-1 \\
            5 &amp; z=-1, y=1\\
            0 &amp; z = y 
        \end{cases}
\end{split}\]</div>
<p>This is an example of an <strong>asymmetrical loss function</strong>: not all wrong predictions are punished equally. On the other hand, symmetrical loss functions like 0-1 loss penalize all incorrect predictions equally.</p>
<p>Consider another example with spam. A false positive is predicting that a good email is spam incorrectly, while a false negative is predicting a spam email is legit. Which is worse? Well, in this case, false positives are much worse- we certainly would rather see spam emails rather than not see real, possibly important emails. So we’d want the <span class="math notranslate nohighlight">\(z=1, y=-1\)</span> case in our loss function to be penalized more than <span class="math notranslate nohighlight">\(z=-1, y=1\)</span>.</p>
</div>
<div class="section" id="risk-is-expected-loss">
<h2>Risk is Expected Loss<a class="headerlink" href="#risk-is-expected-loss" title="Permalink to this headline">¶</a></h2>
<p>Remember that our decision function, also called a <strong>decision rule</strong>, is a function that maps a point in feature space (aka “feature vector”) to <span class="math notranslate nohighlight">\(\pm 1\)</span>: predicts in-class or out-of-class.</p>
<p>The <strong>risk of a decision rule is its expected loss over all values of <span class="math notranslate nohighlight">\(X, y\)</span></strong>. Mathematically:</p>
<div class="math notranslate nohighlight">
\[
R(r) = E[L(r(X), Y)]
\]</div>
<p>We assume there is a <em>joint distribution</em> over values of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and we take the expectation over this distribution. If we know the class-conditional probabilities <span class="math notranslate nohighlight">\(P(X|Y)\)</span>, we can expand this further, by summing over <span class="math notranslate nohighlight">\(X\)</span>’s groups, then <span class="math notranslate nohighlight">\(y\)</span>. You could also do this the other way around: sum over all possible classes in <span class="math notranslate nohighlight">\(y\)</span>, then look at each of <span class="math notranslate nohighlight">\(X\)</span>’s groups that belong to that particular class.</p>
<p>For our cancer example, we first sum over all 3 bins of <span class="math notranslate nohighlight">\(X\)</span> (calorie intake), then for each term sum over all possible classes in <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
R(r) = \sum_{x}(L(r(x),1)P(Y=1|X=x) + L(r(x), -1)P(Y=-1|X=x))P(X=x)
\]</div>
<p>So in all, we sum over all calorie intake groups, and in each group, we sum losses for people who have cancer with that calorie intake and those who don’t have that calorie intake.</p>
<p>We could also do this the other way around: sum over the cancer and non-cancer groups first, then for each, sum the expected loss for each calorie intake group:</p>
<div class="math notranslate nohighlight">
\[
P(Y=1)\sum_{x}L(r(x),1)P(X=x|Y=1) + P(Y=-1)\sum_{x}L(r(x),-1)P(X=x|Y=-1)
\]</div>
</div>
<div class="section" id="bayes-classifier">
<h2>Bayes Classifier<a class="headerlink" href="#bayes-classifier" title="Permalink to this headline">¶</a></h2>
<p>The classifier we want that minimizes this risk <span class="math notranslate nohighlight">\(R(r)\)</span> has a special name: <strong>Bayes’ decision rule</strong>, or the <strong>Bayes classifier</strong>. We usually denote this rule as <span class="math notranslate nohighlight">\(r^*\)</span>. Assuming that loss is 0 for correct predictions (this should always be the case), we can explicitly write the rule as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
r^*(x) = \begin{cases} 
            1 &amp; L(-1, 1)P(Y=1|X=x) &gt; L(1,-1)P(Y=-1|X=x) \\
            -1 &amp; \text{otherwise}
        \end{cases}
\end{split}\]</div>
<p>So really, all we’re doing is comparing posterior probabilities, in essence considering two things: first, the probability of our prediction being correct given evidence, and second, if it worse to have a false positive or a false negative.</p>
<p>Note that in the term <span class="math notranslate nohighlight">\(L(-1, 1)P(Y=1|X=x)\)</span>, we multiply the false negative loss by the probability that our prediction is <em>incorrect</em>- we’re comparing expected losses here. Make sure you understand why this is the case, as it may be unintuitive to see a product of terms like this.</p>
<p>For a symmetric loss function, <span class="math notranslate nohighlight">\(L(1, -1) = L(-1, 1)\)</span>, so all the Bayes classifier is doing is <strong>predicting the class with a higher posterior probability</strong>.</p>
<p>So if we use the Bayes classifier in our cancer example, by plugging in numbers into the classifier formula our classifier comes out to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
r^*(x) = \begin{cases} 
            1 &amp; x \leq 1600 \\
            -1 &amp; x &gt; 1600
        \end{cases}
\end{split}\]</div>
<p>The <strong>Bayes risk</strong>, sometimes called <strong>optimal risk</strong>, is the risk of the Bayes classifier. We already know the formula for calculating the risk of a classifier. So again, just plugging in numbers will give us:</p>
<div class="math notranslate nohighlight">
\[
R(r^*) = P(Y=1)\sum_{x}L(r(x),1)P(X=x|Y=1) + P(Y=-1)\sum_{x}L(r(x),-1)P(X=x|Y=-1)
= 0.1(5 \times 0.3) + 0.9(1 \times 0.01 + 1 \times 0.1) = 0.249
\]</div>
<p>No decision rule will give a risk lower than 0.249, since this classifier is optimal.</p>
<p>The process of deriving and using the classifier <span class="math notranslate nohighlight">\(r^*\)</span> is called <strong>risk minimization</strong>: we are literally finding the classifier that minimizes the risk.</p>
<p>Now suppose we graph the class-conditional probability distributions <span class="math notranslate nohighlight">\(P(X|Y=1)\)</span> and <span class="math notranslate nohighlight">\(P(X|Y=-1)\)</span>:</p>
<img alt="../../_images/classconds.png" src="../../_images/classconds.png" />
<p>But remember that we also must take prior probabilities into account:</p>
<img alt="../../_images/classcondswithprior.png" src="../../_images/classcondswithprior.png" />
<p>Note that such quantities are <em>proportional</em> to our posterior probabilities (since <span class="math notranslate nohighlight">\(P(X)\)</span> is the constant coefficient for both). The intersection of the two curves is the Bayes optimal decision boundary. Remember we just pick the class with the highest posterior probability (or term proportional to it, in this case): we predict cancer for points to the left of the boundary, and no cancer for points to the right.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is possible to have more than one decision boundary, as curves can intersect at multiple points. In the multiple regions that form, the prediction always corresponds to the higher curve.</p>
</div>
</div>
<div class="section" id="risk-in-continuous-distributions">
<h2>Risk in Continuous Distributions<a class="headerlink" href="#risk-in-continuous-distributions" title="Permalink to this headline">¶</a></h2>
<p>Now let’s move to the case where <span class="math notranslate nohighlight">\(X\)</span> is continuous instead of discrete. The definition of risk stays the same, except we need to utilize integrals instead of summations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R(r) = E[L(r(X), Y)] \\
= P(Y=1)\int L(r(x), 1)f(X=x|Y=1)dx + P(Y=-1)\int L(r(x), -1)f(X=x|Y=-1)dx
\end{split}\]</div>
<p>For the Bayes decision rule, the Bayes risk is the area under the curve <span class="math notranslate nohighlight">\(\min\{P(X|Y=1)P(Y=1), P(X|Y=-1)P(Y=-1)\}\)</span>: the minimum of the curve shown above. This risk can be written as a <em>minimization integral</em></p>
<div class="math notranslate nohighlight">
\[
R(r^*) = \int \underset{\pm 1}{\min} L(-y, y)f(X=x|Y=y)P(Y=y)dx
\]</div>
<p>So for each <span class="math notranslate nohighlight">\(x\)</span>, we just choose the “true label” <span class="math notranslate nohighlight">\(y\)</span> that minimizes <span class="math notranslate nohighlight">\(L(-y, y)f(X=x|Y=y)P(Y=y)\)</span> (the graph we plotted), and sum.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we use 0-1 loss, then <span class="math notranslate nohighlight">\(R(r)\)</span> is the general probability that our prediction <span class="math notranslate nohighlight">\(r(x)\)</span> is wrong. Additionally, this also means the Bayes optimal decision boundary is <span class="math notranslate nohighlight">\(\{x: P(Y=1|X=x) = 0.5 \}\)</span>- where a point has a 50-50 chance of being in either class.</p>
</div>
<p>Note the intersection of the two curves is a line- where the Bayes decision boundary lies.</p>
<p>In generating <em>multiclass</em> classifiers, we can actually pretty easily extend this process: just generate one PDF for each class, and simply pick the highest PDF (highest posterior) value for any input <span class="math notranslate nohighlight">\(x\)</span> as a class prediction. In this case, we’ll have <em>multiple</em> decision boundaries.</p>
</div>
<div class="section" id="bayesian-classifier-in-higher-dimensions">
<h2>Bayesian Classifier in Higher Dimensions<a class="headerlink" href="#bayesian-classifier-in-higher-dimensions" title="Permalink to this headline">¶</a></h2>
<p>So far, we’ve just considered one dimension for <span class="math notranslate nohighlight">\(X\)</span> (calorie intake). In the real world, however, <span class="math notranslate nohighlight">\(X\)</span> will have many dimensions. Let’s consider a 2-dimensional feature space. Here is a plot of the (class-conditional) PDFs for points in class C and points not in class C:</p>
<img alt="../../_images/2dccd.png" src="../../_images/2dccd.png" />
</div>
<div class="section" id="ways-to-build-classifiers">
<h2>3 Ways to Build Classifiers<a class="headerlink" href="#ways-to-build-classifiers" title="Permalink to this headline">¶</a></h2>
<p>The Bayes classifier is an example of a <strong>generative model</strong>. In generative models, we assume the sample points come from some probability distribution whose parameters we can estimate. Of course, such distributions are different for each class. Then, for each class C, we fit the distribution parameters to the data in class C, <em>generating</em> the class-conditional distribution <span class="math notranslate nohighlight">\(f(X|Y=C)\)</span>. We must also estimate the priors <span class="math notranslate nohighlight">\(P(Y=C)\)</span> for each class. Once we’ve calculated the class-conditionals and priors, we use Bayes’ Theorem to get posterior probabilities <span class="math notranslate nohighlight">\(P(Y|X)\)</span>.</p>
<p>On the other hand, we also have <strong>discriminative models</strong>. In discriminative models, we skip calculating priors and class-conditionals, and instead try to model the target posteriors <span class="math notranslate nohighlight">\(P(Y|X)\)</span> <em>directly</em>. An example of this is logistic regression, which we’ll cover in a few chapters.</p>
<p>There also exist other models which don’t even care about posterior probabilities and just jump right to calculating the decision function <span class="math notranslate nohighlight">\(r(x)\)</span> directly. We already know of one such algorithm: SVMs.</p>
<p>The advantage of generative and discriminative models is that it tells you the probability that your classification is correct/not. Since we estimate the distribution, generative models are able to detect outliers well: outliers will by definition have a very small <span class="math notranslate nohighlight">\(P(X)\)</span>. However, it is often hard to accurately estimate these distributions.</p>
<p>Generative models tend to be better when data is normally distributed and there are a lot of sample points.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../Ch4/intro.html" title="previous page">Optomization</a>
    <a class='right-next' id="next-link" href="../Ch6/intro.html" title="next page">Gaussian Discriminant Analysis</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kevin Moy<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>