
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Network Variations &#8212; Machine Learning</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mathjax.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Improving Neural Network Training" href="../Ch15/intro.html" />
    <link rel="prev" title="Neural Networks" href="intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/ML_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machapters/Chines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="intro.html">
   Chapter 14: Neural Networks
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Neural Network Variations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch19/intro.html">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch20/intro.html">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch22/intro.html">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch14/intro2.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch14/intro2.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sigmoid-unit-saturation">
   Sigmoid Unit Saturation
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-network-variations">
<h1>Neural Network Variations<a class="headerlink" href="#neural-network-variations" title="Permalink to this headline">¶</a></h1>
<p>Let’s analyze a few basic variations on the standard neural network.</p>
<p>First, remember that neural networks can do regression as well as classification. Unlike classification, neural networks performing regression generally do <em>not</em> have nonlinear activation functions. Do note that in this case the backpropagation algorithm will change as well- fortunately, in a way that makes it simpler.</p>
<p>For classification, if we have two classes, the sigmoid activation function is perfectly fine. What about more than 2 classes, like MNIST? In this case, the <strong>softmax function</strong> might be a better alternative. The softmax function is different in that it is a function of <span class="math notranslate nohighlight">\(k\)</span> outputs, not just one. So now, with <span class="math notranslate nohighlight">\(k\)</span> unique classes, we want <span class="math notranslate nohighlight">\(k\)</span> units in the output layer. Let <span class="math notranslate nohighlight">\(t = Wh\)</span> = the length-<span class="math notranslate nohighlight">\(k\)</span> output units vector before softmax. We then take <span class="math notranslate nohighlight">\(t\)</span> and run it through <span class="math notranslate nohighlight">\(k\)</span> softmax output <span class="math notranslate nohighlight">\(z_j(t) = \frac{e^{t_j}}{\sum_{i=1}^{k}e^{t_i}}\)</span>. Again, there will be <span class="math notranslate nohighlight">\(k\)</span> softmax outputs, each which gives a number in range <span class="math notranslate nohighlight">\(z_j \in (0,1)\)</span>. Of course, this is designed such that the sum of all softmax outputs is 1. We can think of the softmax output, then, as a posterior probability for each class.</p>
<p>Note that the softmax function applied on two classes is essentially equivalent to sigmoid.</p>
<p>Let’s derive backpropagation with softmax outputs. First, we compute the derivative of the softmax output <span class="math notranslate nohighlight">\(z_j\)</span> with respect to its input <span class="math notranslate nohighlight">\(t_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial z_j}{\partial t_j} = z_j(1-z_j)
\]</div>
<p>We also need the derivative of the softmax output with respect to softmax inputs that <em>do not</em> come from it:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial z_j}{\partial t_i} = -z_jz_i
\]</div>
<p>for <span class="math notranslate nohighlight">\(i \neq j\)</span>.</p>
<p>These are the derivatives we need to do backpropagation! But before that, let’s talk about one of the issues with sigmoid outputs.</p>
<div class="section" id="sigmoid-unit-saturation">
<h2>Sigmoid Unit Saturation<a class="headerlink" href="#sigmoid-unit-saturation" title="Permalink to this headline">¶</a></h2>
<p>One issue that comes from using sigmoid is that they can <strong>saturate</strong>. While it does do a good job with keeping other neurons from being able to “overpower” other neurons downstream, the issue is that the output of a unit can get “stuck” if it is too close to 0 or 1.</p>
<p>When unit output <span class="math notranslate nohighlight">\(s\)</span> is close to 0 or 1, that means <span class="math notranslate nohighlight">\(s' = s(1-s) \approx 0\)</span>. This means gradient descent will be extremely slow for that particular neuron. This unit is “stuck”, and can slow down training by quite a bit. Additionally, we can also output a bad local minima!</p>
<p>With more hidden layers, the risk of saturation gets larger. That’s why you’ll find it very rare for image recognition networks, which can have hundreds of hidden layers, to have sigmoid outputs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is commonly referred to as the <strong>vanishing gradient problem</strong>.</p>
</div>
<p>There are a few ways to mitigate the vanishing gradient problem:</p>
<ol class="simple">
<li><p>Initialize weights based on the <em>fan-in</em> of units they connect to. Let’s say we have a NN unit with <em>fan-in</em> <span class="math notranslate nohighlight">\(\eta\)</span>: it has <span class="math notranslate nohighlight">\(\eta\)</span> input edges. We can initialize each incoming edge to a random weight with mean 0, standard deviation <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{\eta}}\)</span>. The larger the fan-in, the easier it is to oversaturate a unit. So for larger fan-ins we need a smaller initial weight.</p></li>
<li><p>Change target values from binary (0/1) to 0.85 and 0.15. If our target is 0/1, then output units will of course tend towards 0/1, and thus be saturated. The region <span class="math notranslate nohighlight">\([0.15, 0.85]\)</span> is the non-saturated region for sigmoid outputs.</p></li>
<li><p>In the case that <em>hidden</em> units are saturated, we can modify backpropagation to add a small constant (typically around 0.1) to <span class="math notranslate nohighlight">\(s'\)</span>. This helps the gradient not be 0. We find that we often find a better descent direction with this than steepest descent!</p></li>
<li><p>Use cross-entropy loss instead of squared error. Now, the gradient actually goes to <em>infinity</em> as predictions <span class="math notranslate nohighlight">\(z_j\)</span> get close to 0 or 1. It is strongly recommended that for any given sample point, its <span class="math notranslate nohighlight">\(k\)</span> labels add up to 1: <span class="math notranslate nohighlight">\(\sum_{i=1}^{k}y_i = 1\)</span>. For example, in MNIST, if we have an input image 9, we assign <span class="math notranslate nohighlight">\(y_9 = 1\)</span> and everything else to 0. You can think of these as posterior probabilities.</p></li>
</ol>
<p>There also exists a cross-entropy loss for sigmoid outputs. For a single sigmoid output <span class="math notranslate nohighlight">\(z\)</span>, we have <span class="math notranslate nohighlight">\(L(z,y) = -y\ln z - (1-y)\ln(1-z)\)</span>.</p>
<p>How can we do backprop for a <span class="math notranslate nohighlight">\(k\)</span> softmax outputs? Well, we need to compute some derivatives. Try these yourself:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial L}{\partial z_j} = -frac{y_j}{z_j} \\
\nabla_{W_i} L = (z_i - y_i)h \\
\nabla_{W_i} L = (z-y)h^T \\
\nabla_hL = W^T(z-y)
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(W_i\)</span> is row <span class="math notranslate nohighlight">\(i\)</span> of weight matrix <span class="math notranslate nohighlight">\(W\)</span>, and <span class="math notranslate nohighlight">\(h\)</span> is our vector of hidden unit outputs. Remember it is <em>absolutely</em> important that <span class="math notranslate nohighlight">\(\sum_{i=1}^{k}y_i = 1\)</span> for these derivations to hold!!</p>
<p>Note the formulas for <span class="math notranslate nohighlight">\(\nabla_{W_i} L\)</span> and <span class="math notranslate nohighlight">\(\nabla_hL\)</span> are true for both softmax and sigmoid outputs.</p>
<p>Now let’s derive backpropagation for a neural network with softmax outputs, cross-entropy loss, and L2 regularization.</p>
<!-- TODO: Run through backprop algorithm, starting at 1:07:00 --></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch14"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">Neural Networks</a>
    <a class='right-next' id="next-link" href="../Ch15/intro.html" title="next page">Improving Neural Network Training</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kevin Moy<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>