
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Boosting &#8212; Machine Learning</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mathjax.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Nearest-Neighbors" href="../Ch21/intro.html" />
    <link rel="prev" title="Multiple Eigenvectors and Random Projection" href="../Ch19/intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/ML_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machapters/Chines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/intro2.html">
     Neural Network Variations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch19/intro.html">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch22/intro.html">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch20/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch20/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaboost">
   Adaboost
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#base-learners-decision-trees">
     Base Learners: Decision Trees
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="boosting">
<h1>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">¶</a></h1>
<div class="section" id="adaboost">
<h2>Adaboost<a class="headerlink" href="#adaboost" title="Permalink to this headline">¶</a></h2>
<p><strong>AdaBoost</strong>, short for “adaptive boosting”, is an <em>ensemble method</em> for classification. Remember that ensemble methods create multiple models (classifiers) and combine their predictions into a final prediction. We can use AdaBoost for classification or regression. It has several key features:</p>
<ul class="simple">
<li><p><strong>Weights</strong> its sample points in training, similar to bagging</p></li>
<li><p>Each <em>base learner</em> is also weighted.</p></li>
<li><p>Weights of misclassified training points are increased.</p></li>
<li><p>More accurate base learners are given more “voting power”</p></li>
</ul>
<p>We’ll analyze what all these mean in this chapter.</p>
<p>First of all, the input to the AdaBoost algorithm is the same as any other classification algorithm: an <span class="math notranslate nohighlight">\(n \times d\)</span> design matrix <span class="math notranslate nohighlight">\(X\)</span> along with associated labels vector <span class="math notranslate nohighlight">\(y \in \mathbb{R}^n\)</span>. We’ll assume labels <span class="math notranslate nohighlight">\(y_i\)</span> are binary: <span class="math notranslate nohighlight">\(y_i = \pm 1\)</span>.</p>
<p>Here are some of the more detailed ideas that characterize AdaBoost:</p>
<ul class="simple">
<li><p>We train <span class="math notranslate nohighlight">\(T\)</span> total classifiers, named <span class="math notranslate nohighlight">\(G_1,...,G_T\)</span>. <span class="math notranslate nohighlight">\(T\)</span> is used because most AdaBoost base learners are decision trees (but they certainly don’t have to be).</p></li>
<li><p>The weight for sample point <span class="math notranslate nohighlight">\(X_i\)</span> in <span class="math notranslate nohighlight">\(G_t\)</span> (some base classifier <span class="math notranslate nohighlight">\(t\)</span>) grows according to how much it was misclassified by the <em>previous iteration’s</em> learners, especially if misclassified by accurate learners.</p></li>
<li><p>For points with larger weights, we train base learners harder to correctly classify those points.</p></li>
<li><p>In the end, we define a <em>metalearner</em> whose predictions are simply a linear combination of the base learners’ predictions.</p></li>
</ul>
<p>Each base learner outputs predictions as <span class="math notranslate nohighlight">\(\pm 1\)</span>, but note that our metalearner outputs a continuous score. Of course, we can easily look at the sign of the continuous prediction and round to <span class="math notranslate nohighlight">\(\pm 1\)</span>.</p>
<p>We learned previously how points with different weights were accounted for in ensemble learners. In regression, we modify the <em>risk function</em> by multiplying each point’s loss function by its weight. In soft-margin SVMs, we multiply each point’s slack by its weight. In boosting, we commonly use decision trees as base learners. Each base decision tree calculates an entropy that decides how to split nodes, which can certainly account for point weights. Instead of computing proportions of points in each class, we now compute proportion of total weight from those points.</p>
<p>Now, in <em>each</em> iteration <span class="math notranslate nohighlight">\(T\)</span>, we have to find the classifier <span class="math notranslate nohighlight">\(G_T\)</span> and the coefficient <span class="math notranslate nohighlight">\(\beta_T\)</span> that minimizes risk (for a given loss function <span class="math notranslate nohighlight">\(L\)</span>).</p>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 21 </span> (AdaBoost optimal classifier + weight)</p>
<div class="definition-content section" id="proof-content">
<p>Find <span class="math notranslate nohighlight">\(G_T\)</span> and <span class="math notranslate nohighlight">\(\beta_T\)</span> such that objective function</p>
<div class="math notranslate nohighlight">
\[
R = \frac{1}{n}\sum_{i=1}^{n}L(M(X_i), y_i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(M(X_i)\)</span> is the prediction of the metalearner: <span class="math notranslate nohighlight">\(M(X_i) = \sum_{i=1}^{T}\beta_t G_t(X_i)\)</span>.</p>
</div>
</div><p>In AdaBoost, our loss function (for the metalearner) is exponential:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L(z, y) = e^{-zy} = \begin{cases} 
      e^{-z} &amp; y = +1 \\
      e^z &amp; y = -1 \\  
   \end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(z\)</span> is our metalearner’s prediction given <span class="math notranslate nohighlight">\(X_i\)</span>, and <span class="math notranslate nohighlight">\(y\)</span> is the truth label of <span class="math notranslate nohighlight">\(X_i\)</span>. Note this loss function is for the <em>metalearner only</em>: base learners can use any loss function they want in training (or might not even have one- e.g. k-NN). Also important to note is that, again, prediction <span class="math notranslate nohighlight">\(z\)</span> is continuous while truth label <span class="math notranslate nohighlight">\(y\)</span> is binary.</p>
<p>The exponential loss function penalizes badly misclassified points more. If we have a lot of outliers, however, we might not want to use this loss function.</p>
<p>One important thing to note is that at any iteration <span class="math notranslate nohighlight">\(T\)</span>, <span class="math notranslate nohighlight">\(T-1\)</span> base learners and <span class="math notranslate nohighlight">\(T-1\)</span> weights have already been trained. We’re only optimizing two: <span class="math notranslate nohighlight">\(\beta_T\)</span> and <span class="math notranslate nohighlight">\(G_T\)</span>.</p>
<!-- TODO: Math for optimizing n*risk -->
<p>So now we want <span class="math notranslate nohighlight">\(G_T\)</span> that minimizes risk- or, in other words, the base learner that minimizes the sum of weights <span class="math notranslate nohighlight">\(w_i^{(T)}\)</span> over all <em>misclassified</em> points <span class="math notranslate nohighlight">\(X_i\)</span>.</p>
<p>Each learner’s weights are dependent on the previous learner’s weights:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
w_i^{(T+1)} = w_i^{(T)}e^{-\beta_T y_i G_T(X_i)} = \begin{cases} 
      w_i^{(T)}e^{-\beta_T} &amp; y_i = G_T(X_i) \\
      w_i^{(T)}e^{\beta_T} &amp; y_i \neq G_T(X_i) \\ 
\end{cases}
\end{split}\]</div>
<p>Notice our point weight shrinks (multiply by <span class="math notranslate nohighlight">\(e^{-\beta_T}\)</span>) if classified correctly, and grows (multiply by <span class="math notranslate nohighlight">\(e^{\beta_T}\)</span>) if misclassified.</p>
<p>Why not just create a (deep) decision tree with 100 percent training accuracy? Of course, overfitting, but there are other reasons as well.</p>
<p>So now, we know how to choose base learner <span class="math notranslate nohighlight">\(G_T\)</span> that minimizes risk. To choose the coefficient <span class="math notranslate nohighlight">\(\beta_T\)</span>, we simply set <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \beta_T}R = 0\)</span>:</p>
<!-- TODO: Math for optimizing risk with respect to weight coefficient-->
<p>This gives us the optimal weight of <span class="math notranslate nohighlight">\(\beta_T = \frac{1}{2}\ln(\frac{1-\text{err_T}}{\text{err}_T})\)</span>.</p>
<p>Now that we have a way to calculate <span class="math notranslate nohighlight">\(G_T, \beta_T\)</span> at each iteration, we can build our optimal metalearner!</p>
<p>Note that if we have a base learner with error rate 0 (perfect classification of training points), then <span class="math notranslate nohighlight">\(\beta_T = \infty\)</span>- and the learner becomes the metalearner. A base learner with training accuracy of 0.5 is a random and completely useless classifier, giving <span class="math notranslate nohighlight">\(\beta_T = 0\)</span>. Notice I said <em>exactly</em> 0.5: it turns out classifiers with training accuracy of, say, 30% is just as useful as a classifier with accuracy 70%! We simply have to flip the sign of predictions. The formula accounts for this as well.</p>
<p>Now, finally, we are ready for the AdaBoost algorithm.</p>
<div class="proof algorithm admonition" id="algorithm-1">
<p class="admonition-title"><span class="caption-number">Algorithm 22 </span> (AdaBoost)</p>
<div class="algorithm-content section" id="proof-content">
<ol class="simple">
<li><p>Initialize weights <span class="math notranslate nohighlight">\(w_i \leftarrow \frac{1}{n}\)</span> for all <span class="math notranslate nohighlight">\(i \in [1,n]\)</span></p></li>
<li><p>Train <span class="math notranslate nohighlight">\(T\)</span> learners. For <span class="math notranslate nohighlight">\(t \leftarrow 1\)</span> to <span class="math notranslate nohighlight">\(T\)</span></p>
<ol class="simple">
<li><p>Train <span class="math notranslate nohighlight">\(G_t\)</span> with weights <span class="math notranslate nohighlight">\(w_i\)</span></p></li>
<li><p>Compute weighted error rate for <span class="math notranslate nohighlight">\(G_t\)</span> as <span class="math notranslate nohighlight">\(\text{err} \leftarrow \frac{\sum_{\text{misclassified}}}{\sum_{all}w_i}\)</span></p></li>
<li><p>Compute coefficient <span class="math notranslate nohighlight">\(\beta_t \leftarrow \frac{1}{2}\ln(\frac{1-\text{err}}{\text{err}})\)</span></p></li>
<li><p>Reweight all points: calculate <span class="math notranslate nohighlight">\(w_i^{(T+1)}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> as given by the formula above.</p></li>
</ol>
</li>
<li><p>Return metalearner <span class="math notranslate nohighlight">\(h(z) = \text{sign}(M(z)) = \text{sign}(\sum_{i=1}^{T}\beta_t G_t(z))\)</span></p></li>
</ol>
</div>
</div><p>Let’s visualize what AdaBoost is doing. First, say we have the below set of points that belong in class C (blue) and not class C (red). We want to build a classifier.</p>
<a class="reference internal image-reference" href="../../_images/adaboost1.png"><img alt="../../_images/adaboost1.png" class="align-center" src="../../_images/adaboost1.png" style="width: 300px;" /></a>
<p>The AdaBoost algorithm is boosting, so we assign weights to each training point as we proceed through training each base learner. Large circles represent points with big weights, small with smaller weights. Suppose we first train a linear classifier that predicts right as class C (blue) and left as not class C (red), which might look like this:</p>
<a class="reference internal image-reference" href="../../_images/adaboost2.png"><img alt="../../_images/adaboost2.png" class="align-center" src="../../_images/adaboost2.png" style="width: 300px;" /></a>
<p>See how incorrectly classified points- points that are a different color than the shaded region they are in- are given bigger weights. Now, since these points are more important to take care of, the next classifier trained might look like this:</p>
<a class="reference internal image-reference" href="../../_images/adaboost3.png"><img alt="../../_images/adaboost3.png" class="align-center" src="../../_images/adaboost3.png" style="width: 300px;" /></a>
<p>So some of the big points from before are smaller, but we <em>still</em> misclassify a few points, so they are bigger than ever. Future iterations will try to take care of these.</p>
<div class="section" id="base-learners-decision-trees">
<h3>Base Learners: Decision Trees<a class="headerlink" href="#base-learners-decision-trees" title="Permalink to this headline">¶</a></h3>
<div class="green admonition important">
<p class="admonition-title">Important</p>
<p>Test</p>
</div>
<p>Most of the time, the base learners in AdaBoost are decision trees. Why trees over others? Additionally, why should these trees be short trees instead of deep, accurate ones? There are many reasons for this.</p>
<p>The first is <em>speed</em>. Remember we have to train multiple base learners. A tree that takes twice as long to classify a point is gonna have that time amplified when it’s the same way with hundreds of base trees. With short trees, they are <em>very</em> fast.</p>
<p>Another reason is we can get very good boosting classifiers without any hyperparameter search. This is much different than many of the classifiers we know: neural nets, SVMs, etc.</p>
<p>Another reason: it is very easy to make a mediocre 55% accuracy tree (just higher than 50%). Once we train enough learners like this, we’ll have a very high accuracy for the metalearner.</p>
<p>Another reason: boosting short trees gives us a lot of <em>bias-variance control</em>. It is possible for boosting to overfit, but because AdaBoost trees are short, we are able to reduce overfitting if needed. As you train more learners, the AdaBoost bias decreases. The AdaBoost variance is more complicated: it often decreases at first, because successive trees focus on di↵erent features, but often it later increases. Overfitting with AdaBoost is kind of unpredictable.</p>
<p>AdaBoost with short trees is a form of subset selection. Features that don’t improve the metalearner’s predictive power enough aren’t used at all. This a means of regularization and also improves running time, especially with many irrelevant features.</p>
<p>Finally, note that boosting linear classifiers gives an approximately linear metalearner, so it’s not a great idea to use linear classifiers as base learners. Methods with nonlinear decision boundaries benefit more from boosting, because they allow boosting to reduce the bias faster. Sometimes you’ll see examples where depth-one decision trees are used with AdaBoost. But depth-one decision trees are linear! Even depth-two decision trees boost substantially better.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch20"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../Ch19/intro.html" title="previous page">Multiple Eigenvectors and Random Projection</a>
    <a class='right-next' id="next-link" href="../Ch21/intro.html" title="next page">Nearest-Neighbors</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Kevin Moy<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>