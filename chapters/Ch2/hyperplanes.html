
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Behind the Decision Boundary &#8212; Machine Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Linear Separability of Data" href="linearseparability.html" />
    <link rel="prev" title="Linear Classifiers" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/ML_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/intro2.html">
     Neural Network Variations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch19/intro.html">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch20/intro.html">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch22/intro.html">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ML_Glossary/intro.html">
   Machine Learning Glossary
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch2/hyperplanes.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch2/hyperplanes.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Behind the Decision Boundary</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="behind-the-decision-boundary">
<h1>Behind the Decision Boundary<a class="headerlink" href="#behind-the-decision-boundary" title="Permalink to this headline">¶</a></h1>
<p>For convenience we’ll show our credit risk classifier again:</p>
<a class="reference internal image-reference" href="../../_images/defaultnew.png"><img alt="Default Dataset" class="align-center" src="../../_images/defaultnew.png" style="width: 400px;" /></a>
<p>What exactly is this red line, and how is it formed?</p>
<p>First of all, many classifiers compute an underlying <strong>decision function</strong> <span class="math notranslate nohighlight">\(f(x)\)</span> that maps a point <span class="math notranslate nohighlight">\(x\)</span> to a scalar such that if <span class="math notranslate nohighlight">\(f(x) &gt; 0\)</span> then <span class="math notranslate nohighlight">\(x\)</span> belongs in class C, and if <span class="math notranslate nohighlight">\(f(x) \leq 0\)</span> then <span class="math notranslate nohighlight">\(x\)</span> doesn’t belong in class C.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Decision functions are also called <em>predictor functions</em> or <em>discriminant functions</em>. The latter is used in more complex contexts, like Gaussian discriminant analysis - we’ll explore what this is in much more depth in a later chapter.</p>
</div>
<p>Then, we can explicitly denote the decision boundary as the <strong>set of all points such that decision function <span class="math notranslate nohighlight">\(f(x) = 0\)</span></strong>. Mathematically, we denote the boundary as set <span class="math notranslate nohighlight">\(\{x \in \mathbb{R}^d : f(x) = 0\}\)</span>. Each point in our set is in <span class="math notranslate nohighlight">\(d\)</span>-dimensional feature space. This set usually forms a hyperplane of dimension <span class="math notranslate nohighlight">\(d-1\)</span> (which lives in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>).</p>
<p>The set <span class="math notranslate nohighlight">\(\{x \in \mathbb{R}^d : f(x) = 0\}\)</span> is also called an <strong>isosurface</strong> of <span class="math notranslate nohighlight">\(f\)</span> for <strong>isovalue</strong> 0. The isovalue is what the function output should be equal to for qualifying points <span class="math notranslate nohighlight">\(x\)</span>. For example, <span class="math notranslate nohighlight">\(\{x \in \mathbb{R}^d : f(x) = 2\}\)</span> is the isosurface of <span class="math notranslate nohighlight">\(f\)</span> with isovalue 2.</p>
<p>To get a better idea of this, let us look at a couple plots of <span class="math notranslate nohighlight">\(f(x,y) = \sqrt{x^2 + y^2} - 3\)</span>:</p>
<a class="reference internal image-reference" href="../../_images/fplot.png"><img alt="Default Dataset" class="align-center" src="../../_images/fplot.png" style="width: 600px;" /></a>
<p>Both plots plot <span class="math notranslate nohighlight">\(f\)</span>, just in different ways. On the left we see our 3D plot and a visualization of what the “actual” function looks like in terms of shape. On the right is a 2D <strong>contour plot</strong>: we are plotting the various isocontours (decision boundaries) of <span class="math notranslate nohighlight">\(f\)</span> and using colors to differentiate different isocontours/values. Note that the isocontours are labeled with their respective isovalues: for example, the decision boundary with isovalue 0 is the third ring out.</p>
<p><strong>Linear classifiers usually use a linear decision function</strong>. There will be some examples that we’ll explore in the next chapter with support vector machines (SVMs): no function but a linear boundary is still produced.</p>
<p>Given that our decision function is <span class="math notranslate nohighlight">\(f(x) = w \cdot x + \alpha\)</span>, our decision boundary is a hyperplane <span class="math notranslate nohighlight">\(H = \{x: w \cdot x = -\alpha\}\)</span>. The coefficients of <span class="math notranslate nohighlight">\(w\)</span> as well as intercept term <span class="math notranslate nohighlight">\(\alpha\)</span> are called the <em>weights</em> or <em>parameters</em> of our linear model, and are learned in the training phase. Let’s establish some intuition in using this hyperplane by proving some important theorems.</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 1 </span> (Orthogonality of <span class="math notranslate nohighlight">\(w\)</span>)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x, y\)</span> be two points that lie on hyperplane <span class="math notranslate nohighlight">\(H\)</span>. Then, <span class="math notranslate nohighlight">\(w \cdot (x-y) = 0\)</span> (where <span class="math notranslate nohighlight">\(w\)</span> is the hyperplane weights that do NOT include intercept term <span class="math notranslate nohighlight">\(\alpha\)</span>).</p>
</div>
</div><p>Proving this is pretty straightforward. Try it yourself first before viewing below:</p>
<div class="proof admonition" id="proof">
<p>Proof. simply multiply out <span class="math notranslate nohighlight">\(w \cdot (x-y)\)</span>.</p>
<p>We know for both of these points, <span class="math notranslate nohighlight">\(w \cdot x + \alpha = w \cdot y + \alpha = 0\)</span></p>
<p>So <span class="math notranslate nohighlight">\(w \cdot (x-y) = w\cdot x - w \cdot y = (-\alpha) - (-\alpha) = 0\)</span></p>
</div>
<p>Remember the identity <span class="math notranslate nohighlight">\(\cos \theta = \frac{x \cdot y}{||x|| \cdot ||y||}\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the angle between vectors <span class="math notranslate nohighlight">\(x,y\)</span>. Because the dot product of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(x-y\)</span> was 0, then <span class="math notranslate nohighlight">\(\cos \theta = 0\)</span> gives <span class="math notranslate nohighlight">\(\theta\)</span> as 90 degrees. <strong>Thus, <span class="math notranslate nohighlight">\(w\)</span> is orthogonal to <span class="math notranslate nohighlight">\(H\)</span> and thus any vector (datapoint) that lies on <span class="math notranslate nohighlight">\(H\)</span></strong>. For this reason, <span class="math notranslate nohighlight">\(w\)</span> is called the <strong>normal vector</strong> of <span class="math notranslate nohighlight">\(H\)</span>.</p>
<p>Note the isovalue chosen will not matter, as these just form parallel hyperplanes, and <span class="math notranslate nohighlight">\(w\)</span> will be orthogonal to all of them.</p>
<!-- TODO: Insert diagram here. -->
<p>If <span class="math notranslate nohighlight">\(w\)</span> is a unit vector (is normalized to a unit vector), then <span class="math notranslate nohighlight">\(w \cdot x + \alpha\)</span> is the <em>signed distance</em> from a data point <span class="math notranslate nohighlight">\(x\)</span> to the hyperplane <span class="math notranslate nohighlight">\(H\)</span>. Of course, this is the line that connects <span class="math notranslate nohighlight">\(x\)</span> to the closest point on <span class="math notranslate nohighlight">\(H\)</span>. Such a distance will be positive if <span class="math notranslate nohighlight">\(x\)</span> is on the same side of the hyperplane as <span class="math notranslate nohighlight">\(w\)</span> (<span class="math notranslate nohighlight">\(w\)</span> points to it) and is negative if on the opposite side.</p>
<div class="proof definition admonition" id="signed-dist">
<p class="admonition-title"><span class="caption-number">Definition 1 </span> (Signed Distance)</p>
<div class="definition-content section" id="proof-content">
<p>Given <span class="math notranslate nohighlight">\(w, a\)</span> that uniquely forms a decision boundary hyperplane <span class="math notranslate nohighlight">\(H\)</span>, the <strong>signed distance</strong> from <span class="math notranslate nohighlight">\(H\)</span> to an arbitrary data point <span class="math notranslate nohighlight">\(x\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\frac{w}{||w||} \cdot x + \frac{\alpha}{||w||}
\]</div>
</div>
</div><!-- TODO: Insert diagram here -->
<p>Moreover, the distance from <span class="math notranslate nohighlight">\(H\)</span> to the origin is <span class="math notranslate nohighlight">\(\alpha\)</span>. We can easily verify this by using our previous proof of signed distance by letting <span class="math notranslate nohighlight">\(x\)</span> = the origin. So then the signed distance from the origin to the hyperplane is <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>Hence we have proved another theorem:</p>
<div class="proof theorem admonition" id="alpha-origin">
<p class="admonition-title"><span class="caption-number">Theorem 2 </span> (<span class="math notranslate nohighlight">\(\alpha\)</span>)</p>
<div class="theorem-content section" id="proof-content">
<p>For any hyperplane <span class="math notranslate nohighlight">\(H\)</span>, <span class="math notranslate nohighlight">\(\alpha = 0\)</span> if and only if <span class="math notranslate nohighlight">\(H\)</span> passes through the origin.</p>
</div>
</div><p>You’ve had experience with this when drawing 1D lines: we know that <span class="math notranslate nohighlight">\(y = cx\)</span> for some constant x doesn’t have an intercept term and will always pass through the origin. This property, of course, extends to many dimensions.</p>
<p>So we’ve covered the basics of the linear classifier in terms of its decision boundary, its supporting decision function, and the parameters <span class="math notranslate nohighlight">\(w, \alpha\)</span> that uniquely define it. Familiarizing yourself with the math in this section might not be fun, but it is important as it will extend to the more complicated classification (and regression!) techniques we’ll see soon.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Linear Classifiers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="linearseparability.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Linear Separability of Data</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Kevin Moy<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>