
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Nearest-Neighbor Algorithms &#8212; Machine Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Machine Learning Glossary" href="../ML_Glossary/intro.html" />
    <link rel="prev" title="Nearest-Neighbors" href="../Ch21/intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/ML_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch4/intro.html">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/intro2.html">
     Neural Network Variations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch19/intro.html">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch20/intro.html">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ML_Glossary/intro.html">
   Machine Learning Glossary
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch22/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch22/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exhaustive-k-nn">
   Exhaustive k-NN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#voronoi-diagrams">
   Voronoi Diagrams
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-d-trees">
   k-d Trees
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Nearest-Neighbor Algorithms</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exhaustive-k-nn">
   Exhaustive k-NN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#voronoi-diagrams">
   Voronoi Diagrams
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-d-trees">
   k-d Trees
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="nearest-neighbor-algorithms">
<h1>Nearest-Neighbor Algorithms<a class="headerlink" href="#nearest-neighbor-algorithms" title="Permalink to this headline">¶</a></h1>
<div class="section" id="exhaustive-k-nn">
<h2>Exhaustive k-NN<a class="headerlink" href="#exhaustive-k-nn" title="Permalink to this headline">¶</a></h2>
<p>The simplest nearest-neighbor algorithm is <em>exhaustive search</em>. Given some <em>query point</em> <span class="math notranslate nohighlight">\(q\)</span>, we search through our training points and find the closest point to <span class="math notranslate nohighlight">\(q\)</span>. We can actually just compute <em>squared distances</em> (not square root) to <span class="math notranslate nohighlight">\(q\)</span>. For <span class="math notranslate nohighlight">\(k=1\)</span>, we pick the nearest point’s class.</p>
<p>What about <span class="math notranslate nohighlight">\(k &gt; 1\)</span>? When <span class="math notranslate nohighlight">\(k\)</span> is large enough, we want to essentially find the <span class="math notranslate nohighlight">\(k\)</span> closest points. We can maintain such distances with a <em>binary max-heap</em> to store the <span class="math notranslate nohighlight">\(k\)</span> shortest distances (seen so far) to <span class="math notranslate nohighlight">\(q\)</span>. Whenever you encounter a sample point closer to q than max-heap root, you remove the heap-top point and insert the better point.</p>
<p>There is <em>zero training time</em> for a k-NN classifier. Everything is dependent on our training sample points and the query time. The query time takes <span class="math notranslate nohighlight">\(O(nd + n \log k)\)</span>: <span class="math notranslate nohighlight">\(O(nd)\)</span> time to scan through design matrix <span class="math notranslate nohighlight">\(X\)</span>, and <span class="math notranslate nohighlight">\(n\)</span> heap operations (insert/remove) which each cost <span class="math notranslate nohighlight">\(\log k\)</span> time (since we keep <span class="math notranslate nohighlight">\(k\)</span> items in the heap at all times).</p>
<p>Can we preprocess <span class="math notranslate nohighlight">\(X\)</span> in a way that gives us query time better than <span class="math notranslate nohighlight">\(O(n)\)</span>? We can!</p>
<ul class="simple">
<li><p>In low-dimensional space (2-5 dimensions), we can construct <em>Voronoi diagrams</em>: … We can then construct a data structure that can search quickly through Voronoi diagram cells.</p></li>
<li><p>In medium-dimensional space (up to about 30 dimensions), we can use <strong>k-d trees</strong>.</p></li>
<li><p>For large dimensions, it’s just best to use exhaustive nearest neighbors and not get cute. Note we can still utilize preprocessing, like random projection or PCA.</p></li>
</ul>
<p>Let’s go into the first two in detail. We’ll start with Voronoi diagrams.</p>
</div>
<div class="section" id="voronoi-diagrams">
<h2>Voronoi Diagrams<a class="headerlink" href="#voronoi-diagrams" title="Permalink to this headline">¶</a></h2>
<p>Think of <span class="math notranslate nohighlight">\(X\)</span> as a set of our training points. Any point <span class="math notranslate nohighlight">\(w \in X\)</span> has a <strong>Voronoi cell</strong>: the Voronoi cell of point <span class="math notranslate nohighlight">\(w\)</span> is the set of all points in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> that are closer to <span class="math notranslate nohighlight">\(w\)</span> as <em>any</em> other point <span class="math notranslate nohighlight">\(p \in X\)</span>. We denote this as</p>
<div class="math notranslate nohighlight">
\[
\text{Vor}(w) = \{p \in \mathbb{R}^d: ||p-w|| \le ||p-v|| \forall v \in X \}
\]</div>
<p>A Voronoi cell will always be a convex polytope. The set of all Voronoi cells for each point in <span class="math notranslate nohighlight">\(X\)</span> makes our <strong>Voronoi diagram</strong>. Let’s look at an example.</p>
<a class="reference internal image-reference" href="../../_images/voronoi.png"><img alt="../../_images/voronoi.png" class="align-center" src="../../_images/voronoi.png" style="width: 400px;" /></a>
<p>Note each green dot represents a training point, and each is surrounded by a Voronoi cell. Every point in each Voronoi cell is closer to its single enclosed green dot than any other training point. So now for a given query point <span class="math notranslate nohighlight">\(q\)</span>, all we have to do is find which Voronoi cell it’s in, find the corresponding enclosed green dot, and we have our nearest neighbor. How can we do this task of <em>point location</em> efficiently?</p>
<p>Naively, we can just look at all <span class="math notranslate nohighlight">\(n\)</span> cells- obviously this is <span class="math notranslate nohighlight">\(O(n)\)</span> runtime. But we can do better with a data structure. In <span class="math notranslate nohighlight">\(d=2\)</span> dimensions, researchers have found data structures constructable in <span class="math notranslate nohighlight">\(O(n \log n)\)</span> time which can do point location in <span class="math notranslate nohighlight">\(O(\log n)\)</span> query time- much better than the standard <span class="math notranslate nohighlight">\(O(n)\)</span> time. Note that computing the Voronoi diagram itself takes <span class="math notranslate nohighlight">\(O(n \log n)\)</span> time. The <em>trapezoidal map</em> is the most popular point location data structure.</p>
<p>In <span class="math notranslate nohighlight">\(d\)</span> dimensions, the data structure to use is a <em>binary search partition tree</em> (BSP trees). They generalize well to high dimensional spaces. Unfortunately, it is more difficult to give a running time for these trees.</p>
<p>Note the biggest problem with Voronoi diagrams: they really only conceptually work for <span class="math notranslate nohighlight">\(k=1\)</span>-NN! <em>Order-k Voronoi diagrams</em> exist for this, but nobody uses this in practice because they are incredibly space-inefficient.</p>
</div>
<div class="section" id="k-d-trees">
<h2>k-d Trees<a class="headerlink" href="#k-d-trees" title="Permalink to this headline">¶</a></h2>
<p>k-d trees essentially mean “k-dimensional trees”: trees that subdivide a d-dimensional feature space (the k is kind of meaningless). They are <em>very</em> similar to decision trees: they still implement axis-aligned cuts of the feature space (cuts by feature). The main differences of k-d trees vs. decision trees:</p>
<ul class="simple">
<li><p>Instead of classification/regression, we use k-d trees to find the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors of a query point.</p></li>
<li><p>The split feature is now based on factors such as feature width, rather than labels/classes (which no longer matter in k-NN). In particular, we want to split on features with <em>longer</em> widths. Alternatively, we can just rotate through features: split on feature 1 at depth 1, feature 2 at depth 2, etc. This makes the tree building runtime faster by a factor of <span class="math notranslate nohighlight">\(O(d)\)</span>.</p></li>
<li><p>The split value is the median point for feature <span class="math notranslate nohighlight">\(i\)</span> OR the midpoint between 2 medians. Choosing the median point helps ensure the tree cannot get too deep: every split halves the number of points, so tree depth cannot exceed <span class="math notranslate nohighlight">\(\log_2 n\)</span>.</p></li>
<li><p>Each internal node stores a sample point. Each node represents a box in feature space, and the sample point lies inside.</p></li>
</ul>
<p>Let’s look at an example of a k-d tree. We will use the “rotating through features” trick: split on x-coordinate median, then y-coordinate, then (recursively) x-coordinate, etc.</p>
<!-- TODO: Walk through construction of k-d tree -->
<p>Note that every node, internal and leaf, corresponds to a feature-aligned box in feature space. The root represents <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, the right child at depth 1 represents the right half-plane, and so on and so forth. It is important to think of nodes and boxes as the same thing.</p>
<p>How do we do NN queries using this tree? For <span class="math notranslate nohighlight">\(k=1\)</span>-NN, given a query point <span class="math notranslate nohighlight">\(q\)</span>, we want to find a sample point <span class="math notranslate nohighlight">\(w\)</span> such that <span class="math notranslate nohighlight">\(||q-w|| \le (1+\epsilon)||q-s||\)</span> where <span class="math notranslate nohighlight">\(s\)</span> is the closest sample point. If we set <span class="math notranslate nohighlight">\(\epsilon = 0\)</span>, we have an exact NN query. But if we set <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, then we have an <em>approximate</em> NN-query. Approximate NN-queries can help speed up queries in high dimensions, trading off for a small amount of imperfection.</p>
<p>So for a query, we don’t want to search through the <em>entire</em> tree if we don’t need to. Our query algorithm maintains two things throughout:</p>
<ul class="simple">
<li><p>Nearest neighbor found so far, or <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors</p></li>
<li><p>Binary min-heap that holds all <em>unexplored subtrees</em> in the k-d tree. All unexplored boxes are keyed by the distance from the query point.</p></li>
</ul>
<p>As we search through the tree, the distance to the nearest neighbor can only go down. As we are exploring</p>
<!-- TODO: Continue at 59:00 -->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch22"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Ch21/intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Nearest-Neighbors</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../ML_Glossary/intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Learning Glossary</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Kevin Moy<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>