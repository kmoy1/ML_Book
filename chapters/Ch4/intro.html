
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Optomization &#8212; Machine Learning</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/qstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/quiz.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Decision Theory" href="../Ch5/intro.html" />
    <link rel="prev" title="Support Vector Machines" href="../Ch3/intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/ML_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation_note.html">
   A Quick Note on Notation and Terminology
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch1/intro.html">
   Chapter 1: Classification and ML Basics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/trainingandtesting.html">
     Training and Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/comparingmodels.html">
     Comparing Classification Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/MNIST.html">
     The MNIST dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/validation.html">
     Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/supervisedlearn.html">
     Supervised vs. Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch1/summary.html">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch2/intro.html">
   Chapter 2: Linear Classifiers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/hyperplanes.html">
     Behind the Decision Boundary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/linearseparability.html">
     Linear Separability of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/centroid.html">
     The Centroid Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptron.html">
     The Perceptron Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp2.html">
     Perceptron Algorithm, Part 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/perceptronp3.html">
     The Perceptron Algorithm, Part 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch2/maxmargin.html">
     Maximum Margin Classfier
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch3/intro.html">
   Chapter 3: Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 4: Optimization Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch5/intro.html">
   Chapter 5: Decision Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch6/intro.html">
   Chapter 6: Gaussian Discriminant Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch7/intro.html">
   Chapter 7: Eigenvectors and Quadratic Forms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch8/intro.html">
   Chapter 8: Anisotropic Gaussians
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9/intro.html">
   Chapter 9: Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch10/intro.html">
   Chapter 10: Statistical Justifications for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch11/intro.html">
   Chapter 11: Regularization
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch12/intro.html">
   Chapter 12: Decision Trees
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch12/intro2.html">
     Lecture 15: Decision Trees Cont.
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch13/intro.html">
   Chapter 13: Kernels
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Ch14/intro.html">
   Chapter 14: Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Ch14/intro2.html">
     Neural Network Variations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch15/intro.html">
   Chapter 15: Improving Neural Network Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch16/intro.html">
   Chapter 16: Unsupervised Learning and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch17/intro.html">
   Chapter 17: Singular Value Decomposition and Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch18/intro.html">
   Chapter 18: Spectral Graph Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch19/intro.html">
   Chapter 19: Multiple Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch20/intro.html">
   Chapter 20: AdaBoost
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch21/intro.html">
   Chapter 21: Nearest Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch22/intro.html">
   Chapter 22: Optomizing k-NN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ML_Glossary/intro.html">
   Machine Learning Glossary
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../../_sources/chapters/Ch4/intro.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/Ch4/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/kmoy1/ML_Book.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/kmoy1/ML_Book.git/issues/new?title=Issue%20on%20page%20%2Fchapters/Ch4/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/kmoy1/ML_Book.git/master?urlpath=tree/docs/chapters/Ch4/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intro">
   Intro
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optomization-problems">
   Optomization Problems
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-to-optimize-smooth-functions">
     Algorithms to Optimize Smooth Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-to-optimize-nonsmooth-functions">
     Algorithms to Optimize Nonsmooth Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#constrained-optimization">
   Constrained Optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-programs">
   Linear Programs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-for-linear-programming">
     Algorithms for Linear Programming
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quadratic-programming">
   Quadratic Programming
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-for-quadratic-program">
     Algorithms for Quadratic Program
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Optomization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intro">
   Intro
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optomization-problems">
   Optomization Problems
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-to-optimize-smooth-functions">
     Algorithms to Optimize Smooth Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-to-optimize-nonsmooth-functions">
     Algorithms to Optimize Nonsmooth Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#constrained-optimization">
   Constrained Optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-programs">
   Linear Programs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-for-linear-programming">
     Algorithms for Linear Programming
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quadratic-programming">
   Quadratic Programming
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithms-for-quadratic-program">
     Algorithms for Quadratic Program
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="optomization">
<h1>Optomization<a class="headerlink" href="#optomization" title="Permalink to this headline">¶</a></h1>
<div class="section" id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Permalink to this headline">¶</a></h2>
<p>Let’s say we have 500 feet of fencing. We want to make a square fence for our house that encloses as much area as possible. What are the dimensions of this square? More than that, if our fence could be <em>any</em> shape, what is the maximum-area figure that we can construct with 500 feet of fencing?</p>
<p>Here’s another cool analogy I came up with. If you have some controllable weights <span class="math notranslate nohighlight">\(w\)</span>, how can you “tune” the individual elements in <span class="math notranslate nohighlight">\(w\)</span> to optimize some quantity? Let’s say you’re a rave DJ: if each weight corresponded to a setting on a DJ board, but a few of the knobs and dials are broken or distorted, what tunings do you set to generate the most happiness out of the crowd? How do you work within the <em>constraints</em> that have been given to maximize production?</p>
<p>In this note we focus on the details behind these kinds of optomization problems. There are a few core types of these problems, along with associated algorithms to solve them. It is by no means a simple subject, but let’s try to break it down here.</p>
</div>
<div class="section" id="optomization-problems">
<h2>Optomization Problems<a class="headerlink" href="#optomization-problems" title="Permalink to this headline">¶</a></h2>
<p>There are two basic kinds of optimization problems: constrained and unconstrained.</p>
<p>Let’s start with <strong>unconstrained optomization</strong>. Generally, this problem involves finding a weight vector <span class="math notranslate nohighlight">\(w\)</span> that minimizes (or maximizes) some <em>continuous</em> objective function <span class="math notranslate nohighlight">\(f(w)\)</span>.</p>
<div class="proof definition admonition" id="contfunc">
<p class="admonition-title"><span class="caption-number">Definition 2 </span> (Continuous Function)</p>
<div class="definition-content section" id="proof-content">
<p>A <strong>continuous function</strong> is a function that can be drawn without <em>discontinuities</em>- ones you can draw without picking up the pencil. More math-ily, a function is continuous when, for every <span class="math notranslate nohighlight">\(c\)</span> in the domain of <span class="math notranslate nohighlight">\(f\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(c)\)</span> is defined</p></li>
<li><p><span class="math notranslate nohighlight">\(\lim_{x \to c}f(x) = f(c)\)</span></p></li>
</ul>
</div>
</div><p>One well-known function that is certainly discontinuous is <span class="math notranslate nohighlight">\(\tan x\)</span>. Note the jumps.</p>
<center>
  <script src="https://www.desmos.com/api/v1.6/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6"></script>
  <div id="calculator" style="width: 300px; height: 300px; text-align: center;"></div>
  <script>
    var elt = document.getElementById('calculator');
    var calculator = Desmos.GraphingCalculator(elt, {expressions:false});
    calculator.setExpression({ id: 'graph1', latex: 'y = \\tan x' });
  </script>
</center>
<p>More than continuous, we hope our objective function <span class="math notranslate nohighlight">\(f\)</span> is <em>smooth</em> as well:</p>
<div class="proof definition admonition" id="smoothfunc">
<p class="admonition-title"><span class="caption-number">Definition 3 </span> (Smooth Function)</p>
<div class="definition-content section" id="proof-content">
<p>A <strong>smooth function</strong> is a continuous function that does not have sharp edges.- ones you can draw without picking up the pencil. Mathematically, this means <span class="math notranslate nohighlight">\(f\)</span> is differentiable at all <span class="math notranslate nohighlight">\(x\)</span>: it also means that <span class="math notranslate nohighlight">\(f\)</span> AND its gradient (derivative) <span class="math notranslate nohighlight">\(\nabla f\)</span> is continuous.</p>
</div>
</div><p>An example of a non-smooth but continuous function is <span class="math notranslate nohighlight">\(|x|\)</span>:</p>
<center>
  <script src="https://www.desmos.com/api/v1.6/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6"></script>
  <div id="calculator2" style="width: 300px; height: 300px; align-items: center; justify-content: center"></div>
  <script>
    var elt = document.getElementById('calculator2');
    var calculator = Desmos.GraphingCalculator(elt, {expressions:false});
    calculator.setExpression({ id: 'graph1', latex: '\\abs (x) ' });
  </script>
</center>
<!-- TODO: How do we prove/know a function is continuous? Smooth? -->
<p>A <strong>global minimum</strong> of <span class="math notranslate nohighlight">\(f\)</span> is denoted as <span class="math notranslate nohighlight">\(\underset{w}{\arg\min} f(w)\)</span>: in other words, <span class="math notranslate nohighlight">\(f(w) \le f(v)\)</span> for all <span class="math notranslate nohighlight">\(v\)</span>. A <strong>local minimum</strong> of <span class="math notranslate nohighlight">\(f\)</span> is a vector <span class="math notranslate nohighlight">\(w\)</span> such that <span class="math notranslate nohighlight">\(f(w) \le f(v)\)</span> AROUND <span class="math notranslate nohighlight">\(w\)</span>: in a tiny ball centered around <span class="math notranslate nohighlight">\(w\)</span>.</p>
<p>Let’s connect this math to something visual. If we take the function <span class="math notranslate nohighlight">\(y = x^6 + x^3 - x^2\)</span>, we’ll see there’s a local minimum at <span class="math notranslate nohighlight">\(x=0.52\)</span>, and a global minimum at <span class="math notranslate nohighlight">\(x = -0.948\)</span>:</p>
<center>
  <script src="https://www.desmos.com/api/v1.6/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6"></script>
  <div id="calculator3" style="width: 500px; height: 300px; align-items: center; justify-content: center"></div>
  <script>
    var elt = document.getElementById('calculator3');
    var calculator = Desmos.GraphingCalculator(elt, {expressions:false});
    calculator.setExpression({ id: 'graph1', latex: 'x^6 + x^3 - x^2'});
  </script>
</center>
<p>Usually, finding a local minimum is easy, but finding the global minimum is generally pretty hard or even impossible. However, the exception for this comes when we have a <strong>convex function</strong>.</p>
<div class="proof definition admonition" id="convexfunc">
<p class="admonition-title"><span class="caption-number">Definition 4 </span> (Convex Function)</p>
<div class="definition-content section" id="proof-content">
<p>A <strong>convex function</strong> is a function such that for every <span class="math notranslate nohighlight">\(x, y \in \mathbb{R}^d\)</span>, the line segment connecting <span class="math notranslate nohighlight">\((x, f(x))\)</span> to <span class="math notranslate nohighlight">\((y, f(y))\)</span> <em>does not go below</em> any <span class="math notranslate nohighlight">\(f(z)\)</span> for any <span class="math notranslate nohighlight">\(z \in [x,y]\)</span>.</p>
</div>
</div><p>A very frequently used example of a convex function is <span class="math notranslate nohighlight">\(y = x^2\)</span>. Note that any line segment we try to draw between points will act as a “lid” to the x-range between those points! Try this yourself.</p>
<img alt="../../_images/convex.png" class="align-center" src="../../_images/convex.png" />
<div class="admonition note" name="html-admonition" style="background: lightgreen; padding: 10px">
<p class="title">This is the **title**</p>
This is the *content*
</div>
<p>The sum of a bunch of convex functions is still convex: therefore, since the risk function is just a sum of a bunch of convex loss functions, the perceptron risk function is convex. However, it is also non-smooth because of the linear constraints it must account for. To prove a function is convex, we simply take its second derivative:
if <span class="math notranslate nohighlight">\(f''(x) \ge 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>, then <span class="math notranslate nohighlight">\(f\)</span> is convex.</p>
<p>In support vector machines, we are minimizing <span class="math notranslate nohighlight">\(||w||^2\)</span>: since this is a quadratic program, it is convex. However, it is not unconstrained, of course.</p>
<p>The reason we like convex functions because on a closed convex domain it must have one of three possibilities:</p>
<ul class="simple">
<li><p>No minimum (goes to -<span class="math notranslate nohighlight">\(\infty\)</span>)</p></li>
<li><p>One local minimum, which must be the global minimum</p></li>
<li><p>Multiple <em>connected</em> local minima: all local minima are global minima.</p></li>
</ul>
<p>Remember that the “green region” we saw with the perceptron risk function indicates that it satisfies the third bullet point: multiple global minima (weight vectors) exist that all linearly separate the data and give global risk <span class="math notranslate nohighlight">\(R(w) = 0\)</span>.</p>
<p>Unfortunately, many applications will result in nonconvex objective functions, and algorithms may just have to settle for local minima. For example, neural network risk functions usually have LOTS of local minima.</p>
<div class="section" id="algorithms-to-optimize-smooth-functions">
<h3>Algorithms to Optimize Smooth Functions<a class="headerlink" href="#algorithms-to-optimize-smooth-functions" title="Permalink to this headline">¶</a></h3>
<p>If our objective function is smooth, then gradient descent is very nice in optomization. There is batch gradient descent, and stochastic gradient descent. However, gradient descent also exists with <em>line search</em>: we are dynamically looking for a minimum at each step, changing step sizes at each iteration.</p>
<p>Another option for optomization is <strong>Newton’s method</strong>. It looks at the <strong>Hessian matrix</strong> of <span class="math notranslate nohighlight">\(f\)</span>, which contains the <em>second-order derivatives</em> of <span class="math notranslate nohighlight">\(f\)</span>. However, the issue is this require a <span class="math notranslate nohighlight">\(d \times d\)</span> matrix for <span class="math notranslate nohighlight">\(d\)</span> features. So if <span class="math notranslate nohighlight">\(d\)</span> is very large, this becomes space-inefficient and intractable very fast.</p>
<p>There also exists a method called the nonlinear conjugate gradient method, which includes line-search methods as part of its algorithm. We won’t talk too much about this in this book, but it is kind of interesting as an algorithm for small neural networks and logistic regression.</p>
</div>
<div class="section" id="algorithms-to-optimize-nonsmooth-functions">
<h3>Algorithms to Optimize Nonsmooth Functions<a class="headerlink" href="#algorithms-to-optimize-nonsmooth-functions" title="Permalink to this headline">¶</a></h3>
<p>Gradient descent is also popular with this. Another algorithm that exists is called BFGS. Such algorithms find a local minimum by the familiar walking-downhill procedure: a general global-minimum finder just doesn’t exist yet.</p>
<p>Line search is like gradient descent, except it implements the process of dynamically trying to find the minimum in the gradient search direction at each iteration. We find a local minimum in the search direction by utilizing a lower-dimensional curve. A few well-known line search methods are the secant method, the Newton-Raphson method, and direct line search.</p>
</div>
</div>
<div class="section" id="constrained-optimization">
<h2>Constrained Optimization<a class="headerlink" href="#constrained-optimization" title="Permalink to this headline">¶</a></h2>
<p>Our goal in constrained optimization is to find <span class="math notranslate nohighlight">\(w\)</span> that minimizes/maximizes <span class="math notranslate nohighlight">\(f(w)\)</span>, <em>subject to</em> constraints <span class="math notranslate nohighlight">\(g(w) = 0\)</span> where <span class="math notranslate nohighlight">\(f, g\)</span> are usually both smooth. If <span class="math notranslate nohighlight">\(g\)</span> is a scalar function (scalar output), then we know <span class="math notranslate nohighlight">\(g(w) = 0\)</span> is an isosurface with isovalue 0. However, if <span class="math notranslate nohighlight">\(g\)</span> is vector-valued, then <span class="math notranslate nohighlight">\(g(w) = 0\)</span> is an <em>intersection with multiple isosurfaces</em>.</p>
<p>The algorithm for solving such problems generally involves something called Lagrange multipliers: they transform a smooth constrained optimization problem and transform it into an unconstrained one. But this is generally beyond the scope of this book.</p>
</div>
<div class="section" id="linear-programs">
<h2>Linear Programs<a class="headerlink" href="#linear-programs" title="Permalink to this headline">¶</a></h2>
<p>A linear program is characterized by a linear objective function and a set of linear constraints. The key fact here is that these constraints might be <em>inequalities</em>. Now, our goal is to find a weight vector <span class="math notranslate nohighlight">\(w\)</span> that optimizes <span class="math notranslate nohighlight">\(f(w) = c \cdot w\)</span>, subject to a set of linear constraints, which can be concisely represented as <span class="math notranslate nohighlight">\(Aw \le b\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(Aw\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are vectors: for a vector <span class="math notranslate nohighlight">\(v_1\)</span> to be  <span class="math notranslate nohighlight">\(\le v_2\)</span>, all elements in <span class="math notranslate nohighlight">\(v_1\)</span> must be <span class="math notranslate nohighlight">\(\le\)</span> their corresponding elements in <span class="math notranslate nohighlight">\(v_2\)</span>.</p>
</div>
<p><span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(\mathbb{R}^{n \times d}\)</span> matrix, <span class="math notranslate nohighlight">\(b \in \mathbb{R}^n\)</span>: this represents <span class="math notranslate nohighlight">\(n\)</span> linear constraints, one for each data point. We can just represent this as <span class="math notranslate nohighlight">\(A_iw \le b_i\)</span> for all <span class="math notranslate nohighlight">\(i \in [1, n]\)</span>, where <span class="math notranslate nohighlight">\(A_i\)</span> is <em>row</em> <span class="math notranslate nohighlight">\(i\)</span> of our matrix <span class="math notranslate nohighlight">\(A\)</span>, representing a data point.</p>
<p>Let’s visualize a linear program and its constraints in 2 dimensions. Remember that a constraint basically shades a section of the (2D) space that our solution is allowed to be in. So if we have <span class="math notranslate nohighlight">\(n = 6\)</span> constraints, our <strong>feasible region</strong>, the “green region” we saw earlier in the perceptron method where our optimal solution is allowed to be in (since it satisfies all constraints), might look like:</p>
<a class="reference internal image-reference" href="../../_images/linearprog.png"><img alt="Linear Program" class="align-center" src="../../_images/linearprog.png" style="width: 500px;" /></a>
<p>The feasible region is a specific version of the <strong>convex polytope</strong> that is produced by the set of all <span class="math notranslate nohighlight">\(w\)</span> that satisfy all constraints. A polytope is a polygon in <span class="math notranslate nohighlight">\(n\)</span> dimensions, usually created by hyperplane constraints. The feasible region does not have to be bounded. However, <strong>the feasible region of a linear program is always convex</strong>. A point set <span class="math notranslate nohighlight">\(P\)</span> is convex if for any 2 points <span class="math notranslate nohighlight">\(p, q \in P\)</span> the line segment that connects <span class="math notranslate nohighlight">\((p, q)\)</span> will be entirely made of points in <span class="math notranslate nohighlight">\(P\)</span> (line lies entirely <em>in</em> <span class="math notranslate nohighlight">\(P\)</span>).</p>
<p>The optimum of a linear program is the point in the feasible region that is <em>furthest in the direction <span class="math notranslate nohighlight">\(c\)</span></em>. Since <span class="math notranslate nohighlight">\(c\)</span> is a direction, we want the point in our region that goes as far in that direction as possible. Another way to think about it: the hyperplane that is orthogonal to <span class="math notranslate nohighlight">\(c\)</span>, when shifted up or down, will intersect the feasible region at exactly one point: this is the optimum.</p>
<p>The optimum will achieve equality for some constraints, but not most. These constraints that achieve equality are called <strong>active constraints</strong> of the optimum. Basically, <strong>all constraint hyperplanes that pass through the optimum are active.</strong> In SVMs, the sample points that induce the 2 active constraints are the support vectors.</p>
<p>Note there can also be multiple or even infinite optimum solutions, depending on <span class="math notranslate nohighlight">\(c\)</span>’s direction.</p>
<p>Note that any feasible point <span class="math notranslate nohighlight">\((w, \alpha)\)</span> gives a linear classifier for linearly separable data. We don’t really prefer one point over another if both of them are optimal: all equally good.</p>
<p>Generally, weight vector points inside the feasible region are better than those on the boundary in terms of linear classifiers, since such resulting hyperplanes don’t touch sample points.</p>
<p>A very important thing to note is that <strong>the data are linearly separable iff the feasible region is not the empty set.</strong> Note that hard-margin SVM feasible regions are a lot easier to visualize than soft-margin SVMs, the latter of which allow for a feasible region with non-linearly-separable data.</p>
<div class="section" id="algorithms-for-linear-programming">
<h3>Algorithms for Linear Programming<a class="headerlink" href="#algorithms-for-linear-programming" title="Permalink to this headline">¶</a></h3>
<p>There exist some algorithms for linear programming that might be interesting to check out:</p>
<ul class="simple">
<li><p>Simplex algorithm: just walk from vertex to vertex in the feasible region, in the direction of optimiziation for <span class="math notranslate nohighlight">\(f\)</span>, until it can’t anymore.</p></li>
<li><p>Interior Point methods</p></li>
</ul>
<p>Note that although linear program solving algorithms can find a linear classifier, they cannot find a maximum margin classifier. We
need something more powerful.</p>
</div>
</div>
<div class="section" id="quadratic-programming">
<h2>Quadratic Programming<a class="headerlink" href="#quadratic-programming" title="Permalink to this headline">¶</a></h2>
<p>In quadratic programming, our objective function is now <em>quadratic</em>, and usually is assumed as convex. We also have the same set of linear inequalities we saw in linear programming. Now, our goal is to find weight vector <span class="math notranslate nohighlight">\(w\)</span> that <em>minimizes</em> <span class="math notranslate nohighlight">\(f(w) = w^TQw + c^Tw\)</span>, still subject to the same linear constraints <span class="math notranslate nohighlight">\(Aw \le b\)</span>. Note <span class="math notranslate nohighlight">\(Q\)</span> is a symmetric positive definite matrix: this means that <span class="math notranslate nohighlight">\(w^TQw \ge 0\)</span> for all <span class="math notranslate nohighlight">\(w \neq 0\)</span>.</p>
<p>The great thing about quadratic programming is that they <strong>only have one local minimum</strong>: therefore, this must be the global minimum. So there’s only one solution as long as <span class="math notranslate nohighlight">\(Q\)</span> is positive definite (and the feasible region is not empty).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Things get really bad if <span class="math notranslate nohighlight">\(Q\)</span> is indefinite- so we’ll assume throughout the book that <span class="math notranslate nohighlight">\(Q\)</span> is positive definite or at least positive semi-definite.</p>
</div>
<p>Of course, one example of quadratic programming is in the maximum margin classifier, where our objective function is minimizing <span class="math notranslate nohighlight">\(||w||^2\)</span>$.</p>
<p>So let’s say we have a plot of a quadratic objective function (where <span class="math notranslate nohighlight">\(Q\)</span> is positive definite). Say we have some feasible region in this plot. The goal, as always, is to find the point in the feasible region that minimizes the objective function. Visually, we want the point in the feasible region that is <em>closest to the origin</em>.</p>
<p>A hard-margin SVM will always have (at least) two active constraints: one for class C and one for class D (not in C). Then, we have two support vectors.</p>
<div class="section" id="algorithms-for-quadratic-program">
<h3>Algorithms for Quadratic Program<a class="headerlink" href="#algorithms-for-quadratic-program" title="Permalink to this headline">¶</a></h3>
<p>Many algorithms also exist for solving quadratic programs:</p>
<ul class="simple">
<li><p>Simplex-like algorithms</p></li>
<li><p>Sequential minimal optimization (SMO)</p></li>
<li><p>Coordinate descent</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/Ch4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Ch3/intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Support Vector Machines</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Ch5/intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Decision Theory</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Kevin Moy<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>